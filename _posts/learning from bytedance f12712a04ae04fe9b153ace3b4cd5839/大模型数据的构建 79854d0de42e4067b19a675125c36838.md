# 大模型数据的构建

现有的大模型（如通义千问、百川之类的具体的训练流程是什么样的？）

现有的大模型的训练语料库是什么样的？语料的配比有哪些？

969:30:1

[https://bytedance.larkoffice.com/space/api/box/stream/download/asynccode/?code=MDY5ZjMzNmNiMmI4NjY0Y2QzNGQ1M2E5YzQ5ZDkzMWFfUklIMXNsZWpJTzBXNXk5RmdKN21HVVlpTjVLb0dVdWFfVG9rZW46RGxJQmJwQUFNb0Flb1N4WVY5eWM0VGZCbkpmXzE3MjE3OTI3MTA6MTcyMTc5NjMxMF9WNA](https://bytedance.larkoffice.com/space/api/box/stream/download/asynccode/?code=MDY5ZjMzNmNiMmI4NjY0Y2QzNGQ1M2E5YzQ5ZDkzMWFfUklIMXNsZWpJTzBXNXk5RmdKN21HVVlpTjVLb0dVdWFfVG9rZW46RGxJQmJwQUFNb0Flb1N4WVY5eWM0VGZCbkpmXzE3MjE3OTI3MTA6MTcyMTc5NjMxMF9WNA)

SFT训练存在的trick有哪些？

如何使用RLHF/DPO的方法来进行一些二次增强？

SFT（PEFT、LLaMA-Factory等方法）的底层代码是如何实现的？

1. LLaMA-Factory要看什么地方？
   1. **如何进行lora的load的？需要准备什么样子的数据？**https://github.com/hiyouga/LLaMA-Factory/blob/main/data/README_zh.md

```jsx

2. **Alpaca 格式**
0.预训练数据集
[
  {"text": "document"},
  {"text": "document"}
]
1. **SFT**
[
  {
    "instruction": "人类指令（必填）",
    "input": "人类输入（选填）",
    "output": "模型回答（必填）",
    "system": "系统提示词（选填）",
    "history": [
      ["第一轮指令（选填）", "第一轮回答（选填）"],
      ["第二轮指令（选填）", "第二轮回答（选填）"]
    ]
  }
]
2. 偏好数据集（DPO、ORPO使用）
[
  {
    "instruction": "人类指令（必填）",
    "input": "人类输入（选填）",
    "chosen": "优质回答（必填）",
    "rejected": "劣质回答（必填）"
  }
]
3.多模态
[
  {
    "instruction": "人类指令（必填）",
    "input": "人类输入（选填）",
    "output": "模型回答（必填）",
    "images": [
      "图像路径（必填）"
    ]
  }
]
```

目的是什么：

1. 如何具体去构建数据集？
    1. Prompt
    2. 用语言模型的能力进行构建
    3. self- instruct/[Evol-Instruct](https://github.com/nlpxucan/evol-instruct) 等通过模型能力的方法去构建数据集（Instrcut、input、output）
        
        <aside>
        💡 *自动扩展Instruction*
        ****• **Self-Instruct**：生成四步骤：构建示例(175个种子任务来抽样8个自然语言指令)来提示InstructGPT生成更多指令→判断是否分类任务+基于给定的“指令”提示InstructGPT生成“输入”再结合生成“输出”→为相应的指令任务生成“输入”和“输出”→后处理(过滤和删除重复)→最终得到52K个英语指令
        • **Evol-Instruct**：包含基于ChatGPT采用进化策略(添加约束、增加推理步骤、复杂化输入等)构建的训练指令和评估指令。形成过程：基于52K的初始集→随机选择1个进化策略让ChatGPT重写指令→过滤未进化的指令对(利用ChatGPT和规则)→利用新生成进化指令对更新数据集→重复上述四次→收集了25万个指令对
        • **WizardLM**（7B）是一种语言模型，通过对由ChatGPT生成的Evol-Instruct指令数据集进行微调，使用LLaMA（7B）完成微调。效果优于alpaca和vicuna。
        • **Self-Augmentation**：通过 <output, instruction> 方式训练模型，对unlabeled data数据进行预测标注，再通过Self-curation方式进行质量筛选，筛选高质量数据再不断迭代训练
        • 通过**self-curation**方法，过滤低质量instruction数据，提升整体数据质量:[https://arxiv.org/pdf/2308.06259.pdf](https://arxiv.org/pdf/2308.06259.pdf)
        • LARGE LANGUAGE MODELS AS OPTIMIZERS：将LLM作为优化器，让LLM逐步地生成指令，来优化目标函数，**找到更优的prompt**[https://arxiv.org/pdf/2309.03409.pdf](https://arxiv.org/pdf/2309.03409.pdf)
        
        </aside>
        
2. **通用instruction优化：FLAN**

<aside>
💡 **FLAN**

- 这个系列工作共三篇文章，第一篇文章是开山之作，第二篇文章扩展了量级（模型、任务数、数据量），第三篇文章是第二篇文章的继承与发扬，给了更细致的拆解、更完善的实验设置。
- 剖析FLAN-T5的成功，作者发现任务的平衡和扩充非常重要：混合提示（zero-shot, few-shot, and chain-of-thought）在所有实验设置下都能收获增益。
- 但是FLAN系列有几个特点需要我们去注意：1）FLAN文章中的技术面向通用领域；2）FLAN的绝大多数实验是在zero-shot或者few-shot情况下讨论的，没有针对具体的下游任务微调。
</aside>

![数据翻转](%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%95%B0%E6%8D%AE%E7%9A%84%E6%9E%84%E5%BB%BA%2079854d0de42e4067b19a675125c36838/output.png)

数据翻转

1. 如何具体的finetune一个base的模型？
    1. SFT->using LLaMA-factory 只需要构建数据集，后面的操作比较傻瓜。

# LLM的Prompt工程分享

> LLM的Prompt工程分享
> 

重点注意LangGPT：

# 数据工程

1. https://mp.weixin.qq.com/s/hbF7ebfbv3PkShxrhUe0bw 数据之间的组合效应：冗余和质量
    1. 多个好的样本数据，但它们编码的知识可能是冗余的和相互冲突的。而由几个相对较低质量但不同的样本组成的另一个数据子集可能比上述子集传递更多的信息。 -> 去重
    2. From Complex to Simple: Enhancing Multi-Constraint Complex Instruction Following Ability of Large Language Models
        1. 1：什么样的数据能增强SFT的效果？复杂约束的数据具有更好的微调结果。也就是数据的描述越详细、准确，微调效果越好。
            1. 2：如何得到这类数据？ Teacher model （修改回答）<-> student model（生成回答） = 正负样本-> DPO
        2. 3：如何利用上述数据进行有效微调？强化学习微调
    3. Scaling Laws for Data Filtering -- Data Curation cannot be Compute Agnostic
        1. 在低计算量的情况下（Epoch数目少），好的数据更好：在低训练计算量的情况下，利用高质量的数据是有益的。但随着Epoch数目的增加，低质量数目集效果更好（可能是见识到的内容更多）。
    4. 注意力集中在头尾：
        1. 通过调整attention
        2. shuffle训练数据？增大model的泛化能力。/ 通过指导，让模型自动去寻找定位。？
2. 

**过去三个月，LLaMA系模型发展如何？指令微调的核心问题又是什么？：**

https://mp.weixin.qq.com/s/cXPNyOeK9vFjJcgxc_LqZQ