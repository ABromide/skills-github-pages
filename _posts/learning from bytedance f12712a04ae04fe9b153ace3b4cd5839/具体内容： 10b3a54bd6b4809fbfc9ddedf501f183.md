# 具体内容：

Qwen大模型结构：

https://www.53ai.com/news/OpenSourceLLM/2024072306721.html

1. **Model Architecture：**
    
    基于 Transformer 架构，带有 causal masks 的 self-attention。
    
    包括 4 个规模的 dense 模型和 1 个 MoE 模型。
    
2. **Qwen2 Dense Model**
    1. 模型架构包括多个 Transformer 层，每层具有 causal attention 机制和 FFN。
        1. **Grouped Query Attention**
        2. **Dual Chunk Attention with YARN**https://blog.csdn.net/qq_27590277/article/details/132750213