# 大模型优化推理

手动分配device

https://mp.weixin.qq.com/s/Aa5Ckwx8y97CRUw9ry1CMQ

# 混合精度

## https://lulaoshi.info/deep-learning/train-speedup/mixed-precision.html#精度-fp32-fp16

混合精度训练的计算流程如下：

1. 参数以FP32存储；
2. 正向计算过程中，遇到FP16算子，需要把算子输入和参数从FP32转换成FP16进行计算；
3. 将Loss层设置为FP32进行计算；
4. 反向计算过程中，首先乘以**Loss Scale**值，避免反向梯度过小而产生下溢；
5. FP16参数参与梯度计算，其结果将被转换回FP32；
6. 除以loss scale值，还原被放大的梯度；
7. 判断梯度是否存在溢出，如果溢出则跳过更新，否则优化器以FP32对原始参数进行更新。

![Untitled](%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E6%8E%A8%E7%90%86%20c04fc1eb9ca045a684bd16a33d33bf9a/Untitled.png)

https://wjn1996.blog.csdn.net/article/details/130764843?spm=1001.2014.3001.5502

![截屏2024-07-25 10.06.39.png](%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E6%8E%A8%E7%90%86%20c04fc1eb9ca045a684bd16a33d33bf9a/%25E6%2588%25AA%25E5%25B1%258F2024-07-25_10.06.39.png)

![截屏2024-07-25 10.19.55.png](%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E6%8E%A8%E7%90%86%20c04fc1eb9ca045a684bd16a33d33bf9a/%25E6%2588%25AA%25E5%25B1%258F2024-07-25_10.19.55.png)

![截屏2024-07-25 10.19.17.png](%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E6%8E%A8%E7%90%86%20c04fc1eb9ca045a684bd16a33d33bf9a/%25E6%2588%25AA%25E5%25B1%258F2024-07-25_10.19.17.png)

deepspeed: 是数据并行方法 → （大部份文章都是讲解重点在于）训练时起作用

模型参数的复制，在用到的 时候 进行复制，用完还给来源，用完就可以删除对应的参数；梯度回传的时候，再次进行参数的复制，梯度计算，梯度汇总，删除临时梯度和参数以及优化器信息。

## 如何具体地去使用Deepspeed：

<aside>
💡

https://zhuanlan.zhihu.com/p/650824387

### **如何选择最佳性能的ZeRO阶段和offload方式**

一般而言，以下规则适用：

从速度角度来看 **Stage 0 (DDP) > Stage 1 > Stage 2 > Stage 2 + offload > Stage 3 > Stage 3 + offloads**

~~从GPU内存使用角度来看 **Stage 0 (DDP) < Stage 1 < Stage 2 < Stage 2 + offload < Stage 3 < Stage 3 + offloads**~~

因此，当想要在适合最少数量的GPU的情况下获得最快的执行速度时，可以遵循以下过程。我们从最快的方法开始，如果遇到GPU内存不足，然后转到下一个速度较慢但使用更少GPU内存的方法，依此类推。

</aside>

# 数据并行、流水线并行、张量并行：

https://zhuanlan.zhihu.com/p/617087561

张量并行（Tensor Parallelism）是一种在深度学习中用于加速模型训练和推理的技术，它通过将模型的权重和计算分布在多个设备（如GPU）上来实现。在张量并行中，1D、2D和3D并行是不同的数据和计算分布方式。

1. **1D并行（一维并行）**：
    - 在1D并行中，模型的权重和计算沿着一个维度被分割和分布。例如，如果有一个深度神经网络，其中的权重矩阵可以被分割成多个部分，每个部分放置在不同的设备上。
    - 这种方法通常用于模型的深度维度，即将不同层的权重分布到不同的设备上。
    - 1D并行相对容易实现，但可能在某些情况下导致负载不均衡，因为某些设备可能需要处理更多的数据。
2. **2D并行（二维并行）**：
    - 2D并行进一步将权重和计算在两个维度上分割。例如，在一个卷积神经网络中，可以同时沿着卷积核的深度和宽度维度进行分割。
    - 这种方法可以更均匀地分布计算负载，因为它允许在多个设备之间更灵活地分配任务。
    - 2D并行通常比1D并行更复杂，因为它需要更精细的同步和通信策略。
3. **3D并行（三维并行）**：
    - 
    
    ![Untitled](%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E6%8E%A8%E7%90%86%20c04fc1eb9ca045a684bd16a33d33bf9a/Untitled%201.png)
    

假设有两个节点**Node1**和**Node2**，每个节点有8个GPU，共计16个GPU。16个GPU的编号分别为Rank0、Rank1、...、Rank15。此外，假设用户设置流水线并行度为**4**，张量并行度为**2**。

**流水线并行**。流水线并行会将整个模型划分为4份，这里称为sub_model_1至sub_model_4。每连续的4张GPU负责一个sub_model。即上图右上角中，**相同颜色的GPU负责相同的sub_model**。

**张量并行**。张量并行会针对流水线并行中的sub_model来进行张量的拆分。即Rank0和Rank1负责一份sub_model_1，Rank2和Rank3负责另一份sub_model_1；Rank4和Rank5负责sub_model_2，Rank6和Rank7负责另一份sub_model_2；以此类推。上图右下角中，**绿色线条**表示单个张量并行组，每个张量并行组都共同负责某个具体的sub_model。

**数据并行**。数据并行的目的是要保证并行中的**相同模型参数读取相同的数据**。经过流水线并行和张量并行后，Rank0和Rank2负责相同的模型参数，所以Rank0和Rank2是同一个数据并行组。上图左上角中的**红色线条**表示数据并行组。

# Megatron-3D并行和Deepspeed

Megatron-3D并行和Deepspeed的ZeRO优化技术都是针对大规模模型训练的解决方案，但它们在设计理念和实现方式上存在一些差异。

首先，Megatron-3D并行是一种模型并行技术，它通过将模型的不同部分分布到多个GPU上来进行训练，从而解决了单机内存限制的问题。Megatron-3D并行主要采用层间模型并行和层内模型并行两种方式，其中层间模型并行是将模型的不同层分配给不同的GPU，而层内模型并行则是将同一层的不同参数分配给不同的GPU。此外，Megatron-3D并行还结合了流水线并行技术，将模型的不同层分配到不同的计算节点上，实现了计算和数据传输的重叠，从而提高了训练速度。

Deepspeed的ZeRO优化技术则是一种内存优化技术，它通过减少每个GPU所需的内存量来支持大规模模型的训练。ZeRO技术的核心是将模型的参数、梯度和优化器状态分散存储在不同的GPU上，而不是让每个GPU存储完整的模型副本。ZeRO分为三个阶段，每个阶段都在前一个阶段的基础上进一步减少内存占用。ZeRO-1阶段主要优化了优化器状态的存储；ZeRO-2阶段进一步优化了模型参数和优化器状态的存储；ZeRO-3阶段则包括了对激活的优化，允许模型大小和训练速度显著提升。

Megatron-3D并行相对于Deepspeed的ZeRO优化技术的优点主要体现在以下几个方面：

1. **更高的灵活性**：Megatron-3D并行可以根据模型的大小和计算资源的情况灵活选择并行方式，既可以使用模型并行，也可以使用数据并行，或者两者结合使用。
2. **更好的扩展性**：Megatron-3D并行通过结合流水线并行和张量并行，能够在大规模集群上实现高效的模型训练，支持千亿级参数量以上的大规模语言模型的分布式训练。
3. **更高的训练效率**：Megatron-3D并行通过数据流优化和混合精度训练等技术手段，提高了模型训练的速度和性能。

而Deepspeed的ZeRO优化技术的优点则在于：

1. **内存优化**：ZeRO通过减少每个GPU上的内存占用，使得可以在有限的硬件资源上训练更大的模型。
2. **易于集成**：ZeRO作为Deepspeed库的一部分，提供了简单的API，易于与现有的深度学习框架集成。
3. **支持超参数优化**：Deepspeed还提供了超参数搜索和自动调优工具，帮助用户找到最佳的训练参数。

总的来说，Megatron-3D并行和Deepspeed的ZeRO优化技术各有优势，选择哪种技术取决于具体的训练需求、模型规模和可用的计算资源。

# **大模型并行训练指南：Megatron-DeepSpeed的模型并行与数据并行**

1. https://blog.csdn.net/v_JULY_v/article/details/132462452

# vLLM

https://zhuanlan.zhihu.com/p/640223710

https://www.lixueduan.com/posts/ai/03-inference-vllm/

https://mp.weixin.qq.com/s/FFcZ1c_a3Ua0vLIj3DGaCQ

<aside>
💡

**PagedAttention**允许从非连续的内存位置高效地检索和使用相关信息。

1. 非连续内存访问和查询机制 
2. 动态内存管理和生成过程
3. 内存共享

**量化**：vLLM 利用 FP16 等量化技术，通过以较低的精度表示 KV 缓存来优化内存使用，从而减少内存占用并加快计算速度。

**连续批处理**：传入的请求被连续批处理在一起，以最大限度地提高硬件利用率并减少计算浪费，最大限度地减少空闲时间。

</aside>

## **手撕LLM- KV Cache**

https://mp.weixin.qq.com/s/E5lK55wI87ylf3cyB7gf0w 

1. 

# AWQ量化

1. 先放大后量化
2. 关键参数保护→激活
3. 网格化搜索

https://zhuanlan.zhihu.com/p/685867596?utm_psn=1751997059807211520

# **Flash attention**

https://courses.cs.washington.edu/courses/cse599m/23sp/notes/flashattn.pdf 

<aside>
💡

1. Safe Softmax —> Online Softmax（加速Softmax计算，只遍历两次） → Flash attention
</aside>

![截屏2024-09-03 14.21.12.png](%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E6%8E%A8%E7%90%86%20c04fc1eb9ca045a684bd16a33d33bf9a/%25E6%2588%25AA%25E5%25B1%258F2024-09-03_14.21.12.png)

![截屏2024-09-03 14.21.56.png](%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E6%8E%A8%E7%90%86%20c04fc1eb9ca045a684bd16a33d33bf9a/%25E6%2588%25AA%25E5%25B1%258F2024-09-03_14.21.56.png)

![截屏2024-09-03 14.23.11.png](%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E6%8E%A8%E7%90%86%20c04fc1eb9ca045a684bd16a33d33bf9a/%25E6%2588%25AA%25E5%25B1%258F2024-09-03_14.23.11.png)

![截屏2024-09-03 14.24.47.png](%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E6%8E%A8%E7%90%86%20c04fc1eb9ca045a684bd16a33d33bf9a/%25E6%2588%25AA%25E5%25B1%258F2024-09-03_14.24.47.png)

# Q-LoRA

<aside>
💡

原文链接：[https://blog.csdn.net/Gefangenes/article/details/131468405](https://blog.csdn.net/Gefangenes/article/details/131468405)

</aside>

首先分析下LoRA微调中的痛点：

1.  参数空间小：LoRA中参与训练的参数量较少，解空间较小，效果相比全量微调有一定的差距。
2.  微调大模型成本高：对于上百亿参数量的模型，LoRA微调的成本还是很高。
3. 精度损失：针对第二点，可以采用int8或int4量化，进一步对模型基座的参数进行压缩。但是又会引发精度损失的问题，降低模型性能。

QLoRA优点：

1. 4-bit NormalFloat：提出一种理论最优的4-bit的量化数据类型，优于当前普遍使用的FP4与Int4。对于正态分布权重而言，一种信息理论上最优的新数据类型，该数据类型对正态分布数据产生比 4 bit整数和 4bit 浮点数更好的实证结果。QLORA包含一种低精度存储数据类型（通常为4-bit）和一种计算数据类型（通常为BFloat16）。在实践中，QLORA权重张量使用时，需要将将张量去量化为BFloat16，然后在16位计算精度下进行矩阵乘法运算。模型本身用4bit加载，训练时把数值反量化到bf16后进行训练。
2. Double Quantization：对第一次量化后的那些常量再进行一次量化，减少存储空间。相比于当前的模型量化方法，更加节省显存空间。每个参数平均节省0.37bit，对于65B的LLaMA模型，大约能节省3GB显存空间。
3. Paged Optimizers：使用NVIDIA统一内存特性，该特性可以在在GPU偶尔OOM的情况下，进行CPU和GPU之间自动分页到分页的传输，以实现无错误的 GPU 处理。该功能的工作方式类似于 CPU 内存和磁盘之间的常规内存分页。使用此功能为优化器状态（Optimizer）分配分页内存，然后在 GPU 内存不足时将其自动卸载到 CPU 内存，并在优化器更新步骤需要时将其加载回 GPU 内存。
4. 增加Adapter：4-bit的NormalFloat与Double Quantization，节省了很多空间，但带来了性能损失，作者通过插入更多adapter来弥补这种性能损失。在LoRA中，一般会选择在query和value的全连接层处插入adapter。而QLoRA则在所有全连接层处都插入了adapter，增加了训练参数，弥补精度带来的性能损失。
————————————

![image.png](%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E6%8E%A8%E7%90%86%20c04fc1eb9ca045a684bd16a33d33bf9a/image.png)