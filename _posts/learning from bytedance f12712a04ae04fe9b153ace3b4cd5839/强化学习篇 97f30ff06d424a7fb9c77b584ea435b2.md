# 强化学习篇

### DPO (Direct Preference Optimization)、RLHF：涉及到Reinforce learning，待学习

DPO：https://www.cnblogs.com/lemonzhang/tag/大模型和强化学习/

[RLHF、DPO、PPO](%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AF%87%2097f30ff06d424a7fb9c77b584ea435b2/RLHF%E3%80%81DPO%E3%80%81PPO%2070b9846646604dbaaf74885c484445b8.md)

[多臂老虎机](https://hrl.boyuai.com/chapter/1/%E5%A4%9A%E8%87%82%E8%80%81%E8%99%8E%E6%9C%BA)：一个直观理解强化学习的例子

马尔可夫奖励过程（MRP）、马尔可夫决策过程（MDP）

1. Q-learning：使用当前action的reward和经验中的最优的Q值进行更新梯度。这会使得Q- learning的算法是一个喜欢冒险的算法，因为每一次的更新都是选择最大的Q值进行更新。而Sarsa则用选的那个Q值对应的的状态当作下一个状态，也就是说明Q-learning的状态更新和梯度更新是可以不一致的。
2. DQN：
    1. Experience Replay（保存每一个动作成为一个数据库）and 目标网络（相当于存储一定的梯度再一次性更新。保证训练过程的稳定性）
3. 策略梯度算法：
    1. 首先通过环境的反馈生成完整的联络，在根据链路从最后一步往回一步一步推，同时更新通过回溯的过程更新梯度。
4. Actor-Critic算法：
    1. Actor 的更新采用策略梯度的原则，Critic可以采取时序差分残差的学习方式
5. RLHF：用不严谨的理解来说的话就是，两部分任务：1。提高评分model的评分。2让winner输出的分布和loser的分布尽可能远
    1. 基础1:强化学习是什么？Agent+Environment+State（前三个一起形成了类似于model等部分）+Action（可以类比为input）+Reward（可以类比为output）；
    2. 基础2:有哪些常见的优化方式：1. Valued-based 离散空间内，通过得到每一个action对应的value，选取最优的action 2.Policy Based 连续action空间内进行预测概率，选取概率最大的action 3.Actor-Critic AC分类就是将Value-Based和Policy-Based结合在一起