# MRZN-LLM面试准备总结笔记

![截屏2024-10-29 14.48.54.png](MRZN-LLM%E9%9D%A2%E8%AF%95%E5%87%86%E5%A4%87%E6%80%BB%E7%BB%93%E7%AC%94%E8%AE%B0%2012e3a54bd6b480618327c42519fb8895/%25E6%2588%25AA%25E5%25B1%258F2024-10-29_14.48.54.png)

| Encoder-Decoder | 双向注意力→长文本生成任务上效果差→偏理解效果好 |
| --- | --- |
| Causal Decoder | 训练效率高，zero-shot 能力更强，具有涌现能力 |
| Prefix Decoder | 训练效率低 |

# 模型训练流程

## Loss

1. Next Token Prediction
    
    ![截屏2024-10-29 14.50.33.png](MRZN-LLM%E9%9D%A2%E8%AF%95%E5%87%86%E5%A4%87%E6%80%BB%E7%BB%93%E7%AC%94%E8%AE%B0%2012e3a54bd6b480618327c42519fb8895/%25E6%2588%25AA%25E5%25B1%258F2024-10-29_14.50.33.png)
    

训练效率：

Prefix Decoder < Causal Decoder
Causal Decoder 结构会在所有token上计算损失，而Prefix Decoder只会在输出上计算损失。

1. 去噪自编码器
    
    随机替换掉一些文本段，训练语言模型去恢复被打乱的文本段，即完形填空，训练目标函数为:
    
    ![截屏2024-10-29 14.51.59.png](MRZN-LLM%E9%9D%A2%E8%AF%95%E5%87%86%E5%A4%87%E6%80%BB%E7%BB%93%E7%AC%94%E8%AE%B0%2012e3a54bd6b480618327c42519fb8895/%25E6%2588%25AA%25E5%25B1%258F2024-10-29_14.51.59.png)
    

## **预训练（Pretraining）**

基础大模型构建了长文本的建模能力，使得模型具有语言生成能力，根据输入的提示词（Prompt），模型可以生成文本补全句子。也有部分研究人员认为，语言模型建模过程中也隐含的构建了包括事实性知识（Factual Knowledge）和常识知识（Commonsense）在内的世界知识（World Knowledge）。

## **有监督微调（Supervised Finetuning）**

LoRA：

**一、显存占用降低**

**被冻结的主干模型参数**：在LoRA训练中，大部分的主干模型参数被冻结，这意味着这些参数不需要在训练过程中进行更新。因此，这些参数所对应的梯度和优化器状态也不需要计算和存储。

**显存占用的减少**：由于被冻结的参数不占用额外的显存用于梯度和优化器状态，LoRA训练时的显存占用显著降低。这使得在相同的硬件条件下，可以使用更大的batch size进行训练，从而提高了训练效率。

**二、计算量减少**

**低秩矩阵的更新**：LoRA通过添加低秩矩阵来近似表示模型的权重更新。这种低秩分解的方法使得在训练过程中只需要更新少量的参数（即低秩矩阵的元素），而不是整个权重矩阵。

**反向传播的简化**：在反向传播过程中，由于只需要计算LoRA权重对应的梯度，因此运算次数大大降低。这进一步减少了计算量，加速了训练过程。

**三、通信时间的减少**

**参数量减少带来的通信优势**：由于LoRA只更新部分参数，因此在多卡训练或分布式训练环境中，需要传输的数据量也相应减少。这减少了通信时间，提高了训练速度。

**四、低精度加速技术的应用**

**量化技术的使用**：LoRA训练过程中可以采用各种低精度加速技术，如FP16、FP8或INT8量化等。这些技术通过降低数据的精度来减少计算量和存储需求，从而进一步加速训练。

综上所述，LoRA通过降低显存占用、减少计算量、减少通信时间以及应用低精度加速技术等方法，实现了对深度学习模型训练过程的加速。这使得LoRA在处理大规模预训练模型时具有显著的优势，尤其是在资源有限的环境下。

## **奖励建模（Reward Modeling）**

## **强化学习（Reinforcement Learning）**

# 模型组成

## **Tokenizer**

tokenizer总体上做三件事情：

1. **分词：**tokenizer将字符串分为一些sub-word token string，再将token string映射到id，并保留来回映射的mapping。从string映射到id为tokenizer encode过程，从id映射回token为tokenizer decode过程。映射方法有多种，例如BERT用的是WordPiece，GPT-2和RoBERTa用的是BPE等等，后面会详细介绍。
2. **扩展词汇表：**部分tokenizer会用一种统一的方法将训练语料出现的且词汇表中本来没有的token加入词汇表。对于不支持的tokenizer，用户也可以手动添加。
3. **识别并处理特殊token：**特殊token包括[MASK], <|im_start|>, <sos>, <s>等等。tokenizer会将它们加入词汇表中，并且保证它们在模型中不被切成sub-word，而是完整保留。

### **GPT族：Byte-Pair Encoding (BPE)**

https://zhuanlan.zhihu.com/p/620508648

假设我们有一个语料库，其中包含单词（pre-tokenization之后）—— old, older,[highest](https://www.zhihu.com/search?q=highest&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22424631681%22%7D), 和 lowest，我们计算这些词在语料库中的出现频率。

假设这些词出现的频率如下：

`{“older”: 3, “finest”: 9, “lowest”: 4}`

让我们在每个单词的末尾添加一个特殊的结束标记“”。因为停止符''/w>"主要表示subword是词后缀。举例来说："st"字词不加""可以出现在词首如"st ar"，加了""表明改字词位于词尾，如"wide st"。

`{“older”: 3, “finest”: 9, “lowest”: 4}`

将每个单词拆分为单个字符，进行第一次统计：

`{:10, o: 7, l:7 d:3, e:16, r:3, f:9, i:9, n:9, s:13, t:13}`， 频次最高的的e,16次

BPE 算法的下一步是寻找最频繁的字符对，进行合并，发现 es 一起出现13次， 则es合并， s消失，e还剩3次，更新后如下：

`{:10, o: 7, l:7 d:3, e:3, r:3, f:9, i:9, n:9, es:13, t:13}`， 频次最高的的es 13次, t 13次。

下一步以此种方法继续进行更新， 发现est可以进一步合并。

`{:10, o: 7, l:7 d:3, e:3, r:3, f:9, i:9, n:9, est:13}`

下一步以此种方法继续进行更新，一步步进行合并更新。算法的停止标准继续迭代直到达到预设的subword词表大小或下一个最高频的字节对出现频率为1。

### **BERT族：Word-Piece**

Word-Piece和BPE非常相似，BPE使用**出现最频繁的组合**构造子词词表，而WordPiece使用**出现概率最大的组合**构造子词词表。

### **多语言支持：Sentence-Piece（ALBERT，XLM-RoBERTa和T5）**

这个包主要是为了多语言模型设计的，它做了两个重要的转化：

1. 以unicode方式编码字符，将所有的输入（英文、中文等不同语言）都转化为unicode字符，解决了多语言编码方式不同的问题。
2. 将空格编码为‘_’， 如'New York' 会转化为['_', 'New', '_York']，这也是为了能够处理多语言问题，比如英文解码时有空格，而中文没有， 类似这种语言区别。

但是，也是因为这两个转化，SentencePiece的tokenizer往往会出现词汇表不全的问题。下面是部分SentencePiece中可能出现的问题：

如果某个token被识别成<unk>，那它就无法与其他也被识别成<unk>的token区分开来。例如在训练的时候有大量{hello world}的样本，在输出的时候就会变成<unk> hello world <unk> 的样本。
这些问题不存在于WordPiece中。这是因为SentencePiece需要对多语言情况进行优化，有些token迫不得已要被删掉。想要加上某些本来tokenizer中不存在的token，可以使用add_tokens()方法。

![image.png](MRZN-LLM%E9%9D%A2%E8%AF%95%E5%87%86%E5%A4%87%E6%80%BB%E7%BB%93%E7%AC%94%E8%AE%B0%2012e3a54bd6b480618327c42519fb8895/image.png)

![image.png](MRZN-LLM%E9%9D%A2%E8%AF%95%E5%87%86%E5%A4%87%E6%80%BB%E7%BB%93%E7%AC%94%E8%AE%B0%2012e3a54bd6b480618327c42519fb8895/image%201.png)

## **Embedding**

tokenize完的下一步就是将token的one-hot编码转换成更dense的embedding编码。

在ELMo之前的模型中，embedding模型很多是单独训练的，而ELMo之后则爆发了直接将embedding层和上面的语言模型层共同训练的浪潮（ELMo的全名就是Embeddings from Language Model）。

我们举个例子来看看embedding层是怎么工作的。在HuggingFace中，seq2seq模型往往是这样调用的：

```jsx
input_ids = tokenizer.encode('Hello World!', return_tensors='pt')
output = model.generate(input_ids, max_length=50)
tokenizer.decode(output[0])
```

例如，如果我们使用T5TokenizerFast来tokenize 'Hello World!'，则：

1. tokenizer会将token序列 ['Hello', 'World', '!'] 编码成数字序列[8774, 1150, 55, 1]，也就是['Hello', 'World', '!', '']，在句尾加一个表示句子结束。
2. 这四个数字会变成四个one-hot向量，例如8774会变成[0, 0, ..., 1, 0, 0..., 0, 0]，其中向量的index为8774的位置为1，其他位置全部为0。假设词表里面一共有30k个可能出现的token，则向量长度也是30k，这样才能保证出现的每个单词都能被one-hot向量表示。
3. 也就是说，一个形状为 (4) 的输入序列向量，会变成形状为 (4, 30k) 的输入one-hot向量。为了将每个单词转换为一个word embedding，每个向量都需要被被送到embedding层进行dense降维。

![image.png](MRZN-LLM%E9%9D%A2%E8%AF%95%E5%87%86%E5%A4%87%E6%80%BB%E7%BB%93%E7%AC%94%E8%AE%B0%2012e3a54bd6b480618327c42519fb8895/image%202.png)

**embedding矩阵的本质是一个查找表，每个单词会定位这个表中的某一行，而这一行就是这个单词学习到的在嵌入空间的语义。**

# **数据**

## **训练数据的来源**[https://mp.weixin.qq.com/s/tfkmxxpMcnznNnGBJFlW0g](https://mp.weixin.qq.com/s/tfkmxxpMcnznNnGBJFlW0g)

**通用数据**（General Data）包括网页、图书、新闻、对话文本等内容[7, 8, 9]。通用数据具有规模大、多样性和易获取等特点，因此可以支持 LLM 的构建语言建模和泛化能力。

1. **网页 Webpage：**ClueWeb09[12]、ClueWeb12[13]、SogouT-16[14]、CommonCrawl
2. **书籍Books**：现有的开源 LLM 研究通常采用 Pile 数据集[15] 中提供的 Books3 和 Bookcorpus2 数据集。
3. **对话数据 Conversation Text：**[PushShift.io](http://pushshift.io/) Reddit[18, 19]、Ubuntu Dialogue Corpus[20]、Douban Conversation Corpus、Chromium Conversations Corpus

专业数据（Specialized Data）包括多语言数据、科学数据、代码以及领域特有资料等数据。通过在预训练阶段引入专业数据可以有效提供 LLM 的任务解决能力。

## **处理方法**

![image.png](MRZN-LLM%E9%9D%A2%E8%AF%95%E5%87%86%E5%A4%87%E6%80%BB%E7%BB%93%E7%AC%94%E8%AE%B0%2012e3a54bd6b480618327c42519fb8895/image%203.png)

## **常见开源数据集合**

[https://mp.weixin.qq.com/s/dvles60DPU6jdGy9pEm0XQ](https://mp.weixin.qq.com/s/dvles60DPU6jdGy9pEm0XQ)

https://mp.weixin.qq.com/s/F60XYsT1dwUBmXN6r2G0eA

# 常见问题：

https://mp.weixin.qq.com/s/zf0JASGQnuXFQwsZqMm0tA

https://mp.weixin.qq.com/s/Kl5l9_NIQtwUgLwiYohE6A

https://mp.weixin.qq.com/s/MB-JntfIjbZ6yYvIJgxweA

https://mp.weixin.qq.com/s/kLI5DPaRW5zDDPmfAG7wrw

https://mp.weixin.qq.com/s/fN5idjo5LHmTE1D8ifz0hA

vllm：https://mp.weixin.qq.com/s/-5EniAmFf1v9RdxI5-CwiQ

[大模型优化推理](../%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E6%8E%A8%E7%90%86%20c04fc1eb9ca045a684bd16a33d33bf9a.md)