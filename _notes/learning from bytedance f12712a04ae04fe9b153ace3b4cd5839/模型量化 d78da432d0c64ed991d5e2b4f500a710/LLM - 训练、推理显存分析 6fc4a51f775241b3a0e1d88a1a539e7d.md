# LLM - 训练、推理显存分析

https://zhuanlan.zhihu.com/p/624740065

## 模型显存计算

以 7B 模型，Attention Block = 32、Dim = 4096 为例:

|  | **参数量** | **精度** | **字节数** | **转换为 KB** | **转换为 MB** | **转换为 GB** |
| --- | --- | --- | --- | --- | --- | --- |
| **Result** | 7000000000 | FP16 | 2 | 13671875.0 | 13351.4404296875 | 13.03 |
| **Result** | 7000000000 | FP32 | 4 | 27343750.0 | 26702.880859375 | 26.07 |

推理时主要考虑模型占用的空间，下面按常规模型大小进行估算:

| **模型精度** | **模型规模** | **Byte** | **最低显存 / G** |
| --- | --- | --- | --- |
| **FP32** | 7B | 4 | 28 |
| **FP32** | 13B | 4 | 52 |
| **FP16** | 7B | 2 | 14 |
| **FP16** | 13B | 2 | 26 |
| **Int8** | 7B | 1 | 7 |
| **Int8** | 13B | 1 | 13 |
| **Int4** | 7B | 1/2 | 3.6 |
| **Int4** | 13B | 1/2 | 6.5 |

## 2.推理显存计算

推理时除了需要考虑模型自身权重外，还需要考虑推理过程中创建的张量所占的内存，我们需要下述信息计算推理额外的显存占用:

- S: Seq Length 序列 token 长度
- B: Batch Size 批次大小
- H: Hidden 隐层维度

输入张量显存计算: B x S x H x Bytes

中间激活值显存: B x S x H x Bytes

- 全部显存粗略计算

```
def calculate_memory_usage(model_size, seq_length, batch_size, hidden_dim, bytes_per_element=4):
    """
    计算显存占用

    :param model_size: 模型参数的数量
    :param seq_length: 序列长度
    :param batch_size: 批次大小
    :param hidden_dim: 隐藏层维度
    :param bytes_per_element: 每个元素的字节数，默认为 4（float32）
    :return: 总显存占用（字节）
    """
    # 模型参数的显存占用
    model_memory = model_size * bytes_per_element

    # 输入张量的显存占用
    input_tensor_memory = batch_size * seq_length * hidden_dim * bytes_per_element

    # 中间激活值的显存占用
    activation_memory = batch_size * seq_length * hidden_dim * bytes_per_element

    # 总显存占用
    total_memory = model_memory + input_tensor_memory + activation_memory

    return total_memory
# 示例参数
model_size = 7000000000  # 模型参数的数量，7B-70亿个参数
seq_length = 8192  # 序列长度
batch_size = 1  # 批次大小
hidden_dim = 4096  # 隐藏层维度
# 计算显存占用
total_memory = calculate_memory_usage(model_size, seq_length, batch_size, hidden_dim)
# 输出结果（以 GB 为单位）
print(f"总显存占用: {total_memory / (1024 ** 3):.2f} GB")

```

[https://bytedance.sg.larkoffice.com/space/api/box/stream/download/asynccode/?code=MTBmZGM3Zjc4M2M1YmNiNGQ5NjA4NjBiNTcwYjNlZWFfOUxBNlY4ZHM5TWViY0VmSWRYTTN3VlF6eGc0b21MMlNfVG9rZW46SDllQ2JKMExab2p5aXV4ODV6WWxzRnJhZ2hlXzE3MjIyNDcwNzg6MTcyMjI1MDY3OF9WNA](https://bytedance.sg.larkoffice.com/space/api/box/stream/download/asynccode/?code=MTBmZGM3Zjc4M2M1YmNiNGQ5NjA4NjBiNTcwYjNlZWFfOUxBNlY4ZHM5TWViY0VmSWRYTTN3VlF6eGc0b21MMlNfVG9rZW46SDllQ2JKMExab2p5aXV4ODV6WWxzRnJhZ2hlXzE3MjIyNDcwNzg6MTcyMjI1MDY3OF9WNA)

- FP32 / Seq = 5120 / bsz = 1 单卡推理

上面运行结果为我们代码粗略估算，下图为 A100 显卡推理实测:

[https://bytedance.sg.larkoffice.com/space/api/box/stream/download/asynccode/?code=ZjJlZDhlZmFjZmFlNDA2YmE5OTdmYTg1MmI0ZmJhMTRfRDRVWTkwOFY1bnBiZXJjSllWWXdMcEE0dU84bGo4T3lfVG9rZW46RHBnSmJFQlRXb3VFb2l4aEtCU2xMaGFkZzVmXzE3MjIyNDcwNzg6MTcyMjI1MDY3OF9WNA](https://bytedance.sg.larkoffice.com/space/api/box/stream/download/asynccode/?code=ZjJlZDhlZmFjZmFlNDA2YmE5OTdmYTg1MmI0ZmJhMTRfRDRVWTkwOFY1bnBiZXJjSllWWXdMcEE0dU84bGo4T3lfVG9rZW46RHBnSmJFQlRXb3VFb2l4aEtCU2xMaGFkZzVmXzE3MjIyNDcwNzg6MTcyMjI1MDY3OF9WNA)

## 3.常规训练显存计算

[https://bytedance.sg.larkoffice.com/space/api/box/stream/download/asynccode/?code=MzE5ZDJmNDFiNjM1YjMxYmZmM2JlODJlYTFkOWM2MGZfYWY0MnNPMTY4QzZkdFZPelR2V09QbWF6S0dLVTZ4d1ZfVG9rZW46UVBVZGJINEhmb1lFWUd4bjFPcWx2RTM0Z25kXzE3MjIyNDcwNzg6MTcyMjI1MDY3OF9WNA](https://bytedance.sg.larkoffice.com/space/api/box/stream/download/asynccode/?code=MzE5ZDJmNDFiNjM1YjMxYmZmM2JlODJlYTFkOWM2MGZfYWY0MnNPMTY4QzZkdFZPelR2V09QbWF6S0dLVTZ4d1ZfVG9rZW46UVBVZGJINEhmb1lFWUd4bjFPcWx2RTM0Z25kXzE3MjIyNDcwNzg6MTcyMjI1MDY3OF9WNA)

以上图为例，训练时占用显存的模块如下:

- 显式占用
    - Parameters: 模型参数
    - Optimizers: 优化器
    - Gradients: 梯度
- 隐式占用
    - Activations: 激活值
    - Memory Buffer: 临时缓存区
    - Memory Fragment: 内存碎片

假设模型的 Parameters 参数量为 $$\phi$$，以 Float32 为例则需要 4$$\phi$$byte 的显存，

训练 Gradients 梯度的参数量也是 $$\phi$$，可以理解为一个参数对应一个自己的梯度，所以需要 4$$\phi$$，

优化器 Optimizer LLM 一般使用 AdamW，其具备一阶动量 momentum 和二阶动量 variance，共计 8$$\phi$$ 参数量:

- $$m_t=\beta_1m_{t-1}+(1-\beta_1)g_t$$
- $$v_t=\beta_2v_{t-1}+(1-\beta_2)g_t^2$$

综上在单精度情况下，模型训练需要的显存量为:

4$$\phi$$ + 4$$\phi$$ + 4$$\phi$$ + 4$$\phi$$ = 16$$\phi$$

同理，如果使用 FP16 计算，则需要的显存降低为 2$$\phi$$，此时需要显存量为 8$$\phi$$ 即 52G，所以训练的显存占用比起推理要大很多。

## 4.LoRA 训练显存计算

- LoRA 训练显存
    - 主体模型的参数为 4$$\phi$$无法省略
    - 主体模型的梯度也无法省略，虽然我们只需要 LoRA 参数部分的梯度，所以梯度的 4$$\phi$$ 也无法省略
    - 但是由于不需要优化主体模型，所以 4$$\phi$$ + 4$$\phi$$ 的 AdamW 优化器的空间我们可以节省，因此这里直接再节省一半显存

以 FP16 为例，此时显存占用就已经下降到 4$$\phi$$ ≈ 26G 了。不过对于多出来的 W = AB 的参数而言，模型参数、梯度、优化器是都需要存储的，但是比起总模型数量已经少很多。

以微调下述层: "q_proj","v_proj","k_proj","o_proj","gate_proj","down_proj","up_proj" 、rank = 8、FP32 为例，可训练参数的比例约为全部参数的 0.4785%，所以需要 8$$\phi $$ 即 52G 的 0.4785% 容量大约 0.25G，所以对显存影响占用不会太大，大头在梯度上。

**结论**:  LoRA 能在显存方面节省主要是因为节省了主体模型的优化器部分的显存。另外，实际场景下，如果我们使用 int-8、int-4 量化的模型，我们的显存还会 1/2、1/4 的显著下降。

- FP16 / rank = 8 / bsz = 1 单卡训练

这里 A100 实测 LoRA 训练显存占用为 24G+ 与我们估算的 26G 大致匹配:

[https://bytedance.sg.larkoffice.com/space/api/box/stream/download/asynccode/?code=OTFiNzU5NTE0N2VjNWY4YmYyNTIxYmE4MTMwY2Y2YjNfNW85Rm1KWWRuMEFtcXc4NFBVanZBZW9kc0Q1TWswMm9fVG9rZW46RmhJbWJtUUlzb0Z6N254aWNvNWxpbUZxZ1ViXzE3MjIyNDcwNzg6MTcyMjI1MDY3OF9WNA](https://bytedance.sg.larkoffice.com/space/api/box/stream/download/asynccode/?code=OTFiNzU5NTE0N2VjNWY4YmYyNTIxYmE4MTMwY2Y2YjNfNW85Rm1KWWRuMEFtcXc4NFBVanZBZW9kc0Q1TWswMm9fVG9rZW46RmhJbWJtUUlzb0Z6N254aWNvNWxpbUZxZ1ViXzE3MjIyNDcwNzg6MTcyMjI1MDY3OF9WNA)

## 5.快速计算所需显存

- Hugging Face 官方提供一个界面可以快速计算模型尺寸大小以及训练显存:

```
此工具将帮助您计算在 🤗 Hugging Face Hub 上托管的模型上训练和执行大型模型推理所需的 vRAM 数量。模型所需的最小建议 vRAM 表示为“最大层”的大小，模型的训练大约是其大小的 4 倍（对于 Adam）。这些计算最多精确到几个百分点，例如 bert-base-cased 为 413.68 MB，计算器估计为 413.18 MB。在执行推理时，预计会在此基础上再增加 20%，正如 EleutherAI 发现的那样。未来将进行更多测试，以获得每个模型更准确的基准。目前，此工具支持使用 transformer 和 timm 托管的所有模型。要使用此工具，请传入您要计算内存使用量的模型的 URL 或模型名称，选择它来自哪个框架（“auto”将尝试从模型元数据中检测它），以及您要使用的精度。

```

[https://bytedance.sg.larkoffice.com/space/api/box/stream/download/asynccode/?code=MzYxOGYyNDI1NzlmZjEwNzNhOTk5Nzc0NDU0NzA5MGFfbVFsM2NUUE1CWTFvMEJKeWVRNVRTV2FoSHRtenhXb3lfVG9rZW46UVdxMmJJN3JNb25Zamx4Zkc3Q2x4ZGc3Z3I1XzE3MjIyNDcwNzg6MTcyMjI1MDY3OF9WNA](https://bytedance.sg.larkoffice.com/space/api/box/stream/download/asynccode/?code=MzYxOGYyNDI1NzlmZjEwNzNhOTk5Nzc0NDU0NzA5MGFfbVFsM2NUUE1CWTFvMEJKeWVRNVRTV2FoSHRtenhXb3lfVG9rZW46UVdxMmJJN3JNb25Zamx4Zkc3Q2x4ZGc3Z3I1XzE3MjIyNDcwNzg6MTcyMjI1MDY3OF9WNA)

可以看到 Model 和 Gradient 的容量相同，最终 Optimizer Step 所占容量约为 Model 容量的 4 倍，与上面结论一致。

网址: https://huggingface.co/spaces/hf-accelerate/model-memory-usage