# 策略面试准备

# 最小二乘估计和极大似然估计

最小二乘估计和极大似然估计是两种在统计学和机器学习中常用的参数估计方法，它们具有各自的特点和适用场景。以下是它们的相同点和不同点：

### **相同点**

1. **目标**：两者都旨在通过给定的样本数据来估计模型的参数，使得模型能够更好地拟合或描述这些数据。
2. **应用场景**：两者在回归分析、机器学习等领域都有广泛的应用。在回归分析中，最小二乘估计常用于线性回归，而极大似然估计则更广泛地应用于各种概率模型，包括线性回归（在误差项服从正态分布时）和其他类型的回归模型。

### **不同点**

1. **原理**：
    - **最小二乘估计**：基于误差最小化的原则，通过最小化预测值和真实值之间的平方差来估计参数。其目标函数是残差平方和（SSE），即预测值和真实值之差的平方和。
    - **极大似然估计**：基于概率最大化的原则，通过找到一组参数，使得这组参数下观测到当前数据的概率最大。其目标函数是似然函数，通常表示为观测数据在给定参数下的联合概率分布。
2. **求解过程**：
    - **最小二乘估计**：在简单线性回归中，可以通过解析方式直接求得参数的值，即求解线性方程组或最小化目标函数（SSE）的偏导数并令其等于零。
    - **极大似然估计**：通常需要使用迭代算法（如梯度下降、牛顿法等）来求解参数，因为似然函数可能比较复杂，难以直接求解。
3. **对数据的假设**：
    - **最小二乘估计**：不需要对数据分布进行假设，只要求数据具有有限的方差。
    - **极大似然估计**：需要知道数据的概率分布函数（或假设其满足某种分布，如正态分布），以便构造似然函数。
4. **适用范围**：
    - **最小二乘估计**：适用于线性回归和某些非线性回归问题，特别是当目标是预测一个连续的输出时。
    - **极大似然估计**：适用于各种概率模型，包括线性回归（在误差项服从正态分布时）和其他类型的回归模型，以及分类问题（如逻辑回归）。

综上所述，最小二乘估计和极大似然估计在原理、求解过程、对数据的假设以及适用范围等方面存在显著差异。在实际应用中，应根据具体问题和数据特点选择合适的参数估计方法。

# 假设性检验

在假设性检验中，第一类错误和第二类错误是两种可能的判断失误，它们分别定义如下：

### **第一类错误（Type I Error）**

第一类错误，又称“拒真错误”或“以真为假”，是指原假设（H0）实际上是正确的，但检验的结果却错误地拒绝了原假设。其发生的概率通常用α表示，也称为显著性水平（significance level）。例如，在统计学中，如果我们设定α=0.05，这意味着在100次假设检验中，我们预期有5次可能会错误地拒绝一个实际上正确的原假设。

### **第二类错误（Type II Error）**

第二类错误，又称“纳伪错误”或“以假为真”，是指原假设（H0）实际上是不正确的，但检验的结果却未能拒绝原假设，从而接受了它。其发生的概率通常用β表示。与第一类错误不同，第二类错误的概率β通常不能预先确定，它的大小受多种因素影响，如参数的实际值与假设值之间的距离、样本容量以及检验水准α等。

在假设检验中，我们通常希望同时减少第一类错误和第二类错误的概率，但这两者之间存在一种权衡关系。在其他条件不变的情况下，如果要求犯第一类错误的概率越小（即α值越小），那么犯第二类错误的概率往往会越大；反之亦然。因此，在实际应用中，我们需要根据问题的具体背景和重要性来设定合适的α值，并尽可能通过增加样本容量、改进实验设计等方法来提高检验的准确性和可靠性。

# SVM的核函数

支持向量机（SVM）的核函数是机器学习领域中的一个核心概念，它主要用于将数据从原始的低维空间映射到一个更高维度的空间中，从而使得在高维空间中可以更容易地找到线性分割的决策边界。以下是对SVM核函数的详细介绍：

### **一、核函数的基本概念**

核函数（Kernel Function）是一个定义在输入空间的两个样本之间的函数，其输出为这两个样本在某个高维特征空间中的内积。核函数可以看作是在高维空间中，两个点之间的“相似性”或“关系”的度量。通过核函数，复杂的数据变得可以被简单的方法（比如直线或平面）来分类。

### **二、核函数的性质**

核函数通常具有以下性质：

1. **封闭性**：核函数对于输入向量的计算是封闭的，即两个向量的内积等于它们在特征空间中的向量之积。
2. **唯一性**：不同的核函数会生成不同的特征空间，因此核函数的选择会影响机器学习算法的性能和结果。
3. **计算高效性**：核函数应该能够快速计算，以便在算法中有效地运用。

### **三、常用的核函数及其计算方法**

1. **线性核函数**：
    - 特点：在高维空间中构造一个线性分类器，使得数据点在高维空间中被线性划分。
    - 优点：计算简单，不需要进行复杂的矩阵运算。
    - 缺点：只能处理线性可分的数据集，对于非线性数据集可能无法得到较好的分类效果。
2. **径向基函数（RBF）核函数**：
    - 又称高斯核函数，适用于处理高维数据和实现非线性映射。
    - 表达式：通常表示为K(x,z)=exp(-γ||x-z||²)，其中x和z是输入向量，γ是核参数。
    - 特点：能够将数据映射到无穷维的特征空间，使得线性不可分的数据在高维空间中变得线性可分。
    - 缺点：计算开销较大，训练时间较长。
3. **多项式核函数**：
    - 适用于处理低维数据，通过将输入向量的坐标进行多项式展开来计算内积。
    - 表达式：通常表示为K(x,z)=(x·z+c)^d，其中x和z是输入向量，c是常数项，d是多项式的次数。
    - 优点：能够处理非线性可分的数据集，且计算相对简单。
    - 缺点：当多项式的次数过高时，可能会导致过拟合现象。

### **四、核函数的选择建议**

1. **根据数据类型和特征选择合适的核函数**：
    - 对于高维数据和复杂结构，可以选择RBF核函数。
    - 对于低维数据和简单结构，可以选择多项式核函数。
2. **根据任务类型选择合适的核函数**：
    - 对于分类任务，可以选择SVM中常用的RBF核函数或多项式核函数。
    - 对于回归任务，可以选择高斯过程回归中常用的RBF核函数或Sigmoid核函数。
3. **使用交叉验证等方法来评估不同核函数的性能**：
    - 在实际应用中，可以通过交叉验证等方法来评估不同核函数的性能，并选择最优的核函数。

综上所述，SVM的核函数是机器学习中的一个重要工具，它能够将数据映射到高维空间并解决非线性分类、回归和降维等任务。在选择核函数时，需要根据数据类型、任务类型和性能要求等因素进行综合考虑。

# **logistics regression推导**

本质上是使得现实数据发生的概率最高

https://zhuanlan.zhihu.com/p/44591359

# LSTM

### LSTM的门结构和作用

LSTM（长短期记忆网络）是一种特殊的循环神经网络（RNN），它通过引入门控机制来解决传统RNN在处理长期依赖关系时的梯度消失问题。LSTM的核心组件包括三个门：遗忘门（Forget Gate）、输入门（Input Gate）和输出门（Output Gate）。

1. **遗忘门（Forget Gate）**：决定哪些信息应该从细胞状态中遗忘。它通过sigmoid函数输出一个0到1之间的值，0表示完全遗忘，1表示完全保留。
2. **输入门（Input Gate）**：决定哪些新信息将存储在细胞状态中。它包括两部分：sigmoid函数确定要更新的部分，以及tanh函数产生新的候选值，可能添加到状态中。
3. **输出门（Output Gate）**：决定下一个隐藏状态应该是什么。首先，输出门使用sigmoid激活函数来决定细胞状态的哪些部分将输出，然后这个值与细胞状态的tanh激活的值相乘得到最终输出。

### GRU的门结构和作用

GRU（门控循环单元）是LSTM的一个变体，它简化了LSTM的结构，只包含两个门：更新门（Update Gate）和重置门（Reset Gate）。

1. **重置门（Reset Gate）**：决定了如何将新的输入信息与前面的记忆相结合。
2. **更新门（Update Gate）**：定义了前面记忆保存到当前时间步的量。

GRU通过这两个门控机制来控制信息的流动，类似于LSTM，但是结构更简单，参数更少。

### LSTM与GRU的区别

1. **门的数量**：LSTM有三个门（输入门、遗忘门和输出门），而GRU只有两个门（更新门和重置门）。
2. **结构复杂度**：由于GRU只有两个门，因此其计算复杂度相对较低，这使得GRU在训练速度和计算资源方面具有优势。
3. **记忆单元**：LSTM中有一个单独的记忆单元（cell state）来存储长期记忆，而GRU没有单独的记忆单元，而是直接在隐藏状态中进行更新。
4. **参数数量**：GRU的参数数量比LSTM少，这使得GRU在某些情况下训练更快，尤其是在小数据集上。
5. **性能**：在超参数全部调优的情况下，GRU和LSTM的性能相当，但在某些特定任务上，LSTM可能表现更好，尤其是在需要处理更复杂序列依赖性的任务上。

总结来说，LSTM和GRU都是强大的循环神经网络变体，能够处理长期依赖问题，但它们在结构和性能上有所不同，选择哪一个通常取决于具体任务和数据。

# PCA（主成分分析）和因子分析

PCA（主成分分析）和因子分析是两种常用的降维技术，它们在数据分析和机器学习中有着广泛的应用。下面我将分别介绍这两种方法：

### PCA（主成分分析）

PCA是一种统计方法，它通过正交变换将一组可能相关的变量转换为一组线性不相关的变量，称为主成分。这些主成分按照方差的大小排序，方差越大的主成分保留了更多的原始数据信息。

**主要步骤：**

1. **标准化数据**：由于PCA对数据的尺度敏感，因此首先需要对数据进行标准化处理，使得每个特征的均值为0，方差为1。
2. **计算协方差矩阵**：计算数据的协方差矩阵，以确定变量之间的相关性。
3. **计算特征值和特征向量**：对协方差矩阵进行特征分解，得到特征值和对应的特征向量。
4. **选择主成分**：根据特征值的大小，选择前k个最大的特征值对应的特征向量，这些特征向量构成了主成分。
5. **构造新的特征空间**：将原始数据投影到选定的主成分上，形成新的特征空间，实现降维。

**优点**：

- 可以减少数据集的维度，同时保留最重要的信息。
- PCA是一种线性变换，计算简单，易于实现。

**缺点**：

- PCA假设最大的方差是最重要的特征，这可能不总是正确的。
- PCA对异常值敏感，异常值可能会影响主成分的方向。

### 因子分析

因子分析是一种统计方法，用于描述观察到的变量之间的变异性，以更少的不可观测的变量（称为因子）来表示。因子分析试图解释变量之间的相关性，通过构建潜在的因子来减少数据的复杂性。

**主要步骤：**

1. **数据准备**：与PCA类似，因子分析也需要对数据进行标准化处理。
2. **构建相关矩阵或协方差矩阵**：计算变量之间的相关性或协方差。
3. **提取因子**：使用特定的提取方法（如主成分分析、最大似然法等）来确定因子。
4. **旋转因子**：为了使因子更具有解释性，通常会对因子进行旋转（如正交旋转或斜交旋转）。
5. **因子得分**：计算每个观测值在每个因子上的得分，这些得分可以用于后续分析。

**优点**：

- 因子分析可以揭示变量之间的潜在关系，有助于理解数据的潜在结构。
- 因子分析可以处理非线性关系，而PCA只能处理线性关系。

**缺点**：

- 因子分析的结果依赖于旋转方法的选择，不同的旋转可能导致不同的解释。
- 因子分析不如PCA那样直接，解释因子可能比较困难。

**总结**：
PCA和因子分析都是降维技术，但它们的目的和方法有所不同。PCA通过正交变换提取主成分，而因子分析试图找到解释变量之间相关性的潜在因子。在实际应用中，选择哪一种方法取决于数据的特性和分析的目标。

# 特征构造和特征优化

https://www.showmeai.tech/article-detail/208

特征构造和特征优化是特征工程中的两个重要环节，它们对于提高机器学习模型的性能至关重要。以下是一些特征构造和特征优化的方法：

### 特征构造

特征构造主要是指从原始数据中产生衍生变量，即通过对原始数据进行加工、特征组合，生成有商业意义的新变量（新特征）。

1. **统计值构造法**：通过统计单个或多个变量的统计值（如最大值、最小值、平均值等）来形成新的特征。
    - 单变量：如果某个特征与目标高度相关，可以取这个特征的统计值作为新特征。
    - 多变量：如果特征之间存在交互影响，可以聚合分组两个或多个变量之后，再以统计值构造出新的特征。
2. **函数变换法**：对数据进行数学函数变换，如对数变换、幂次变换等，以提取有用的信息。
3. **算术运算构造法**：通过算术运算（加、减、乘、除）构造新特征。

### 特征优化

特征优化是指对已有特征进行处理，以提高模型的预测表现能力。以下是一些特征优化的策略：

1. **特征选择**：通过选择最有用的特征来减少模型的复杂性和过拟合，同时防止模型的拟合不足。
    - 过滤法（Filter）：按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。
    - 包装法（Wrapper）：根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。
    - 嵌入法（Embedded）：先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。
2. **降维**：降低特征矩阵的维度，减少计算量，提高训练效率。常见的降维方法包括主成分分析（PCA）和线性判别分析（LDA）。
3. **特征抽取**：从原始数据抽取新特征，使用算法自动执行，目的是将多维的或相关的特征降低到低维，以提取主要信息或生成与目标相关性更高的信息。
4. **数据预处理**：包括无量纲化（如标准化和归一化）、处理缺失值、信息利用率低等问题。
5. **特征增强**：通过数据清洗技巧，如清除空值、日期转换等，增强特征的表达能力。

通过这些特征构造和特征优化的方法，可以有效地从原始数据中提取出对模型更有用的特征，从而提高机器学习模型的性能。

## xgb模型中哪些特征比较重要？

XGBoost模型中的重要特性主要包括以下几个方面：

1. **特征重要性评估**：XGBoost提供了几种方法来评估特征的重要性，包括`weight`、`gain`和`cover`三种指标。
    - **Weight（权重）**：表示特征在所有树中被用作分割样本的总次数。如果一个特征被频繁地用于分割数据，那么它的权重就会更高，表明该特征在模型中的重要性越高。
    - **Gain（增益）**：表示特征在其出现过的所有树中产生的平均增益。增益是XGBoost在构建决策树时，通过比较分裂前后的目标函数变化来计算的。如果一个特征的增益值较高，那么它对模型的贡献就越大。
    - **Cover（覆盖）**：表示特征在其出现过的所有树中的平均覆盖范围。覆盖范围是指与该特征相关的观测的相对数量。如果一个特征在多个树的叶节点中用于决定大量的观测值，那么它的覆盖范围就会更大。

## LR的损失函数可以用MSE吗？

逻辑回归（LR）通常用于二分类问题，其标准的损失函数是交叉熵损失（Cross-Entropy Loss），也称为对数损失（Log Loss）。交叉熵损失函数衡量的是模型预测概率与实际发生事件之间的差异，对于二分类问题，其公式为：

\[ L(y, p) = -[y \log(p) + (1 - y) \log(1 - p)] \]

其中，\( y \) 是实际标签（0或1），\( p \) 是模型预测为正类的概率。

然而，理论上可以使用均方误差（Mean Squared Error, MSE）作为损失函数来训练逻辑回归模型。MSE损失函数计算的是预测值与实际值之间差的平方的平均值，其公式为：

\[ L(y, \hat{y}) = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 \]

其中，\( y_i \) 是第i个样本的实际标签，\( \hat{y}_i \) 是模型预测的值。

尽管可以使用MSE作为损失函数，但这并不常见，原因如下：

1. **概率解释**：逻辑回归的输出是概率，而MSE损失函数通常用于连续值预测，不适用于概率输出。
2. **风险敏感性**：MSE对异常值敏感，而交叉熵损失对所有错误都给予相同的权重。
3. **梯度问题**：当使用MSE作为损失函数时，逻辑回归的梯度可能会非常小，导致学习过程缓慢，特别是在概率接近0或1的情况下。
4. **预测范围**：逻辑回归的输出应该在0和1之间，而MSE损失函数不强制输出保持在这个范围内。
5. **统计一致性**：交叉熵损失函数是最大似然估计的结果，对于二分类问题，它是统计上一致的损失函数。

因此，尽管技术上可以使用MSE作为逻辑回归的损失函数，但出于上述原因，实践中通常不这么做。交叉熵损失函数更适合于处理概率输出，并且与逻辑回归模型的性质更加匹配。

## XGBoost在计算梯度时使用二阶泰勒展开的优势

XGBoost在计算梯度时使用二阶泰勒展开的优势主要体现在以下几个方面：

1. **自定义损失函数的支持**：通过使用二阶泰勒展开，XGBoost能够支持自定义损失函数。这意味着，只要损失函数是二阶可导的，就可以应用XGBoost的优化框架。这种灵活性使得XGBoost可以处理包括回归和分类在内的多种机器学习任务。
2. **提升模型的可扩展性**：二阶泰勒展开的使用增强了XGBoost的可扩展性。因为对于任何二阶可导的损失函数，都可以复用XGBoost关于最小二乘法的推导，这样就不需要为每种损失函数重新推导优化算法和参数选择，从而简化了代码实现，并提高了算法的通用性。
3. **优化计算效率**：XGBoost通过二阶泰勒展开得到的梯度和Hessian信息，可以更精确地估计损失函数的下降方向，从而加速模型的收敛速度。这种近似方法使得XGBoost在每一步迭代中都能更有效地优化模型参数。
4. **统一损失函数求导形式**：XGBoost使用泰勒展开来统一不同损失函数求导的形式，使得对于不同的损失函数（如MSE、对数损失等）可以使用统一的框架进行优化。这种统一的形式允许XGBoost将针对MSE的优化方法复用到其他自定义损失函数上，提高了算法的适用性和灵活性。
5. **减少过拟合**：XGBoost的目标函数中加入了正则化项，通过二阶泰勒展开可以更精确地控制模型的复杂度，从而减少过拟合现象的发生。

综上所述，XGBoost中使用二阶泰勒展开的主要优势在于其能够提供自定义损失函数的支持、增强模型的可扩展性、提升计算效率、统一损失函数求导形式以及减少过拟合，这些特性共同使得XGBoost成为一个强大而灵活的机器学习工具。