# 决策树

https://blog.csdn.net/GreenYang5277/article/details/104500739

![image.png](%E5%86%B3%E7%AD%96%E6%A0%91%2012e3a54bd6b4801aac76cd93b40d4b16/image.png)

```jsx
from sklearn.tree import DecisionTreeClassifier #导入分类模型
from sklearn.tree import DecisionTreeRegressor  #导入回归模型

model_c = DecisionTreeClassifier(max_depth=10,max_features=5) #括号内加入要人工设定的参数
model_r = DecisionTreeRegressor(max_depth=10,max_features=5)  #同样的，加入参数设定值，不仅局限于这几个

model_c.fit(x_train,y_train)  #训练分类模型
model_r.fit(x_train,y_train)  #训练回归模型

result_c = model_c.predict(x_test)  #使用模型预测分类结果
result_r = model_r.predict(x_test)  #使用模型预测回归结果

```

决策树的原理主要基于概率论和图论，是一种归纳学习方法，用于从无序、杂乱的数据中提炼出可以预测未知数据的树状模型。以下是对决策树原理的详细解释：

### 一、定义与结构

决策树（Decision Tree）是一种树形数据结构，其中每个内部节点表示一个属性上的测试，每个分支代表一个测试输出，每个叶节点代表一种类别或预测值。决策树通过一系列“if-then”决策规则的集合，将特征空间划分成有限个不相交的子区域，对于落在相同子区域的样本，决策树给出相同的预测值。

### 二、基本原理

1. **假设空间**：决策树模型的关键是将特征空间划分成不相交的子区域，落在相同子区域的样本具有相同的预测值。假设空间即我们对模型形式的先验假设，最终求得的模型必定符合我们对模型形式的先验假设。
2. **目标函数**：目标函数决定了从假设空间中选择模型的偏好。决策树的目标函数可以用来评价一棵决策树的好坏，包括反应决策树对样本数据点拟合准确度的损失项和反应决策树模型复杂程度的正则化项。
3. **优化算法**：优化算法决定了用什么样的步骤在假设空间中寻找合适的模型。对于决策树而言，优化算法包括树的生成策略和树的剪枝策略。树的生成策略一般采用贪心的思想不断选择特征对特征空间进行切分。

### 三、关键概念与度量

1. **熵（Entropy）**：熵是对某个离散随机变量不确定性大小的一种度量。在决策树的应用场景中，用经验熵来衡量标签取值分布的“纯度”，即用频率分布代替概率分布进行计算。
2. **条件熵**：给定随机变量X的取值的前提下，随机事件Y的不确定性的一种度量。在决策树的应用场景中，条件熵的含义更加清晰明了，即按照离散特征X的取值将样本空间划分成多个叶子节点，各个叶子节点上样本标签Y取值的熵不纯度的加权平均。
3. **信息增益**：随机变量X对于随机变量Y的信息增益被定义成Y的熵和Y对X的条件熵之差。在决策树的应用场景中，信息增益的含义就是特征X对样本标签Y不确定性减少的贡献。ID3算法采用信息增益作为待分裂特征的选择标准。
4. **增益比**：为了解决信息增益倾向于选择特征取值数量较多的特征的问题，增益比被提出。增益比是信息增益除以属性a的固有属性IV(a)，能在一定程度上抑制信息增益偏好取值多的属性的特点。C4.5算法使用信息增益和增益比两种选择算法。
5. **基尼不纯度**：基尼不纯度和熵具有相似的作用，可以衡量一个随机变量取值的不确定性或者说“不纯净”程度。基尼不纯度增益和信息增益的作用非常类似，CART决策树使用基尼不纯度作为划分准则。

### 四、生成与剪枝

1. **生成策略**：决策树的生成策略一般采用贪心的思想，即每次选择当前最优的特征进行划分，直到满足停止条件（如所有样本都属于同一类别、所有特征都已使用等）。
2. **剪枝策略**：剪枝是决策树停止分支的方法之一，包括预剪枝和后剪枝两种。预剪枝是在树的生长过程中设定一个指标，当达到该指标时就停止生长；后剪枝则是在树完全生成后，根据一定的规则进行修剪。剪枝的目的是防止决策树过拟合，提高模型的泛化能力。

综上所述，决策树的原理是通过一系列“if-then”决策规则将特征空间划分成有限个不相交的子区域，并对每个子区域给出相同的预测值。在构建决策树的过程中，需要选择最优的划分特征和划分点，并通过剪枝策略防止过拟合。