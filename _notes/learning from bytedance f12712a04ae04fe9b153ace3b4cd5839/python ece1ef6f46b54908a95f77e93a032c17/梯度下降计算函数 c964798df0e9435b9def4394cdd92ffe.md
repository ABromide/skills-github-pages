# 梯度下降计算函数

```jsx
import torch

# 定义目标函数，使用torch张量
def f(x):
    return x**2 + torch.exp(x)

# 梯度下降算法
def gradient_descent(y_target, initial_x, learning_rate, tolerance, iterations):
    x = torch.tensor([initial_x], requires_grad=True)  # 需要梯度的张量
    for _ in range(iterations):
        y = f(x)
        loss = y - y_target  # 定义损失函数
        loss.backward()  # 反向传播，计算梯度
        ***x.data*** -= learning_rate * x.grad  # 更新x的值
        ***x.grad.zero_()***  # 清空梯度，为下一次迭代准备
        if abs(***y.item() - y_target***) < tolerance:  # 检查是否达到精度要求
            break
    return x.item()

# 设置参数
y_target = 20  # 目标y值
initial_x = 1   # 初始x值
learning_rate = 0.1  # 学习率
tolerance = 1e-5  # 精度阈值
iterations = 1000  # 迭代次数

# 执行梯度下降算法
x_solution = gradient_descent(y_target, initial_x, learning_rate, tolerance, iterations)
print(f"The solution for y = 20 is approximately x = {x_solution}")
```