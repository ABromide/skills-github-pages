# 大模型入门p1

本次分享以代码讲解和实操为主，假设受众为LLM的小白。

# 模型结构（5min）

## LLM的基本概念

LLM （生成式大语言模型）的本质就是一个文本表征+生成的模型，text2text的模型，更广泛地可以推广到token2token模式。

暂时无法在飞书文档外展示此内容

相关的几个概念：

| **场景** | **输入** | **输出** |
| --- | --- | --- |
| chat | 问题 | 回答text |
| chat-In-Context Learning | 当前问题+历史问答 | 回答text |
| multiModal | 文本Prompt+其他模态TokenEmbedding | 回答text |
| One-shot Learning | 问题：Prompt/Prompt+TaskDescription | 回答text |
| RAG | 问题：Prompt+RetrivalTextParagraph | 回答text |
| agent+functioncalling | 问题->理解并调用系统工具->工具返回结果+问题 | 回答text |

## LLM的基本结构

### Transformer

[https://bytedance.larkoffice.com/space/api/box/stream/download/asynccode/?code=MGE1ZTFkMjlkMDdhNGI5M2Y3NTY5M2Y1MjkwMGQzNGZfZGFxUURDZnNwQWhqOGlKZzdmMUN6WDNJQ2Zmb1BOVFdfVG9rZW46SkdzM2I2ODh1b3lxSzd4MVJPSmMzUDFRbkNnXzE3MjY5MTgzNDc6MTcyNjkyMTk0N19WNA](https://bytedance.larkoffice.com/space/api/box/stream/download/asynccode/?code=MGE1ZTFkMjlkMDdhNGI5M2Y3NTY5M2Y1MjkwMGQzNGZfZGFxUURDZnNwQWhqOGlKZzdmMUN6WDNJQ2Zmb1BOVFdfVG9rZW46SkdzM2I2ODh1b3lxSzd4MVJPSmMzUDFRbkNnXzE3MjY5MTgzNDc6MTcyNjkyMTk0N19WNA)

### Encoder Only

Bert为典型的EncoderOnly结构，模型的使命是对文本进行语法/语义的学习和理解，预训练任务为MLM（Masking Language Modeling）、NSP（Next Sentence Prediction）。

### Encoder-Decoder

T5、早期的GLM都是Encoder-Decoder结构。GLM（General Language Model Pretraining with Autoregressive Blank Infilling）基于自回归空白填充预训练框架。类似于BERT，从输入文本中随机地空白出连续的跨度的token（片段），并按照自回归预训练的思路，训练模型来依次重建片段。

### Decoder Only

当前最流行的LLM框架，GPT（Generative Pre-trained Transformer）、LLAMA、QWEN都是基于decoder-only的。使用自回归任务进行预训练（**自回归：使用当前时刻的输入来预测下一个时刻的输出**）。

[https://bytedance.larkoffice.com/space/api/box/stream/download/asynccode/?code=MzcyNGMwYTZlMmQ4OTdlNmJlNzI3ZjljOWQ0MWMxMjNfekNDUm1Udk9POXJRT1BoZmd2SGVKeHhrWlBoUHVaZkxfVG9rZW46TXNlbmI2aGhEb1hSNVR4WDFlMmNEWUkwbmZlXzE3MjY5MTgzNDc6MTcyNjkyMTk0N19WNA](https://bytedance.larkoffice.com/space/api/box/stream/download/asynccode/?code=MzcyNGMwYTZlMmQ4OTdlNmJlNzI3ZjljOWQ0MWMxMjNfekNDUm1Udk9POXJRT1BoZmd2SGVKeHhrWlBoUHVaZkxfVG9rZW46TXNlbmI2aGhEb1hSNVR4WDFlMmNEWUkwbmZlXzE3MjY5MTgzNDc6MTcyNjkyMTk0N19WNA)

# 编码实现（30min）

## Tokenizer

```
from transformers import AutoTokenizer, AutoModel

# tokenizer = AutoTokenizer.from_pretrained('Alibaba-NLP/gte-Qwen2-1.5B-instruct', trust_remote_code=True,  padding_side='left')
tokenizer = AutoTokenizer.from_pretrained(
    '/mnt/bn/account-mllm-lf1/BLM_MODEL/seqformer/seqformer0p5b',
    trust_remote_code=True,
    padding_side='left',
    use_fast=True)
max_token_len = 100
input_text = '明月几时有, 把酒问青天，不知天上宫阙'
output = tokenizer.encode_plus(input_text, max_length=max_token_len, padding=True, return_tensors='pt', truncation=True)
print(output['input_ids'].shape, output['attention_mask'].shape)
print(f"******"*10)
full_tokens = tokenizer.batch_decode(output['input_ids'], skip_special_tokens=True, clean_up_tokenization_spaces=False)
print(f"输入Text: {input_text}")
print(f"分词后结果: {output['input_ids']}")
print(f"解码后结果: {full_tokens}")

```

## CausalAttention

[https://bytedance.larkoffice.com/space/api/box/stream/download/asynccode/?code=ZDkyZTgyMjdjZmMwNTIwOWM2Njc2NDZhNDIwMGNiZTVfYXFxbGJlWWIwcmJWSTRFanU3U1JMR0FyTXNoYm5Gd3hfVG9rZW46RjhCRGJkdlZqb2dUeGx4MVhjYWNRV0VqblJmXzE3MjY5MTgzNDc6MTcyNjkyMTk0N19WNA](https://bytedance.larkoffice.com/space/api/box/stream/download/asynccode/?code=ZDkyZTgyMjdjZmMwNTIwOWM2Njc2NDZhNDIwMGNiZTVfYXFxbGJlWWIwcmJWSTRFanU3U1JMR0FyTXNoYm5Gd3hfVG9rZW46RjhCRGJkdlZqb2dUeGx4MVhjYWNRV0VqblJmXzE3MjY5MTgzNDc6MTcyNjkyMTk0N19WNA)

举例，文本“我喜欢吃西瓜”

| step | labels→Input↓ | [BOS] | 我 | 喜 | 欢 | 吃 | 西 | 瓜 | [EOS] |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 1 | [BOS] | ⭕️ | √ |  |  |  |  |  |  |
| 2 | [BOS]我 | ⭕️ | ⭕️ | √ |  |  |  |  |  |
| 3 | [BOS]我喜 | ⭕️ | ⭕️ | ⭕️ | √ |  |  |  |  |
| 4 | ...... | ⭕️ | ⭕️ | ⭕️ | ⭕️ | √ |  |  |  |
| 5 |  | ⭕️ | ⭕️ | ⭕️ | ⭕️ | ⭕️ | √ |  |  |
| 6 |  | ⭕️ | ⭕️ | ⭕️ | ⭕️ | ⭕️ | ⭕️ | √ |  |
| 7 |  | ⭕️ | ⭕️ | ⭕️ | ⭕️ | ⭕️ | ⭕️ | ⭕️ | √ |

```
class CausalAttention(nn.Module):
"""    因果注意力    """def __init__(
            self,
            heads,  # 自注意力的头数
            head_dim,  # 单头embed_size
            embed_dim,  # hidden_size
            dropout=0.0,
    ):
        super(CausalAttention, self).__init__()
        self.heads = heads
        self.head_dim = head_dim
        self.emb_dim = embed_dim
        assert embed_dim == head_dim * heads
        self.dropout = dropout
        # 通过wordEmbedding-->扩展得到QKV矩阵(N, 1, embed_dim) -> (N, 1 3*emb_dim)
        self.qkv_projection = nn.Linear(self.emb_dim, 3*self.emb_dim)
        self.dropout1 = nn.Dropout(self.dropout)
        self.dropout2 = nn.Dropout(self.dropout)
        self.ffn = nn.Linear(self.emb_dim, self.emb_dim)
        pass

    def forward(self, hidden_states, attention_mask):
""":param hidden_states: (N, seqLen, embed_dim), Context hidden_state:param attention_mask: (N, seqLen) --> (N, 1, seqLen, seqLen), 0: valid,:return: attention之后的hidden_state        """bs, seq_len, _ = hidden_states.size()
        qkv = self.qkv_projection(hidden_states)
        # 每个QKV的形状为 (N, seqLen, embed_dim=heads*head_dim)
        q, k, v = torch.split(qkv, qkv.size(-1)//3, dim=-1)
        # 将q,k,v划分为多头 (N, seqLen, head_dim)  --> (N, seqLen, heads, head_dim) --> (N, heads, seqLen, head_dim)
        q = q.view(bs, seq_len, self.heads, self.head_dim).transpose(1, 2).contiguous()
        k = k.view(bs, seq_len, self.heads, self.head_dim).transpose(1, 2).contiguous()
        v = v.view(bs, seq_len, self.heads, self.head_dim).transpose(1, 2).contiguous()
        # 注意力计算-点积注意力: Attention = softmax[(Q*K')/sqrt(d)]*V
        # QK': (N, heads, seqLen, head_dim) * (N, heads, head_dim, seqLen) --> (N, heads, seqLen, seqLen)
        #      (N, heads, i, seqLen): 表示位置为i的token对所有的seqLen个token的注意力
        logits = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(self.heads)
        logits = logits.to(torch.float32)
        # 构造上三角阵形式的mask，实现causal效果: 等价于将对于位置i的token而言将位置i+1之后的所有logits全部置为无穷小
        mask = torch.tril(torch.ones(seq_len, seq_len, dtype=torch.long)).to(logits.device)
        # ******** mask.shape=torch.Size([208, 208])|torch.int64, logits.shape=torch.Size([32, 8, 208, 208])|torch.float16
        # print(f"******** mask.shape={mask.shape}|{mask.dtype}, logits.shape={logits.shape}|{logits.dtype}")
        logits = logits.masked_fill(mask == 0, -1e19)
        # print(f"============= logits.shape={logits.shape}||{attention_mask.shape}")
        # logits = logits.masked_fill(attention_mask == 0, -1e12)
        logits = torch.nn.functional.softmax(logits, dim=-1)
        logits = self.dropout1(logits)
        # (N, heads, seqLen) *  (N, heads, seqLen, head_dim) --> (N, heads, seqLen, head_dim) --> (N, seqLen, embed_dim)
        weighted_v = torch.matmul(logits, v).transpose(1, 2).contiguous().view(bs, seq_len, -1).contiguous()
        weighted_v = self.ffn(weighted_v)
        weighted_v = self.dropout2(weighted_v)
        return weighted_v
    pass

```

## DecoderLayer

单个解码层主要由LN+CausalAttention+FFN构成，多个DecoderLayers堆叠在一起构成完整的Decoder。

```
class DecoderLayer(nn.Module):
"""    解码层: 对于当前位置为i的token，对0-i范围的context进行编码，生成i+1及之后新token的预测概率 -- 自回归生成过程    Note: 最终层需要生成预测概率，但中间层则只需要做CasualAttention生成表示即可    结构:        Input_X  -----------        |                   |        LN                  |        |                   |        CausalAttention ------        |                     |        LN                    |        |                     |        FFN ------------------        |        Activation    """def __init__(
            self,
            heads,  # 自注意力的头数
            head_dim,  # 单头embed_size
            embed_dim,  # hidden_size
            ffn_expand=3,  # FFN中上采样比例
            ffn_dropout=0.0,
            attn_dropout=0.0,
            max_seq_len=512
    ):
        super(DecoderLayer, self).__init__()
        self.embed_dim = embed_dim
        self.ffn_expand = ffn_expand
        self.ffn_dropout = ffn_dropout
        self.ln1 = nn.LayerNorm(self.embed_dim)
        self.ln2 = nn.LayerNorm(self.embed_dim)
        self.casual_attention = CausalAttention(
            heads,
            head_dim,
            embed_dim,
            attn_dropout
        )
        self.act = nn.GELU()
        self.ffn = nn.Sequential(
            nn.Linear(embed_dim, self.ffn_expand * embed_dim),
            nn.GELU(),
            nn.Linear(self.ffn_expand * embed_dim, embed_dim),
            nn.Dropout(self.ffn_dropout))

    def forward(self, x, attention_mask):
""":param x: HiddenState (N, seqLen, emb_dim):return: HiddenState (N, seqLen, emb_dim)        """x1 = self.ln1(x)
        x1 = self.casual_attention(x1, attention_mask)
        x = x1 + x
        x2 = self.ffn(x)
        x = x2 + x
        x = self.act(x)
        return x
    pass

```

## LLMModel

- Decoder为模型主体
- 添加nextToeknPrediction作为损失函数

```

class ChatLLM(nn.Module):
"""    模型结构: 采用DecodeOnly结构，直接采用多层解码器堆叠即可    并封装: 推理预测功能    self.tokenizer = AutoTokenizer.from_pretrained('Alibaba-NLP/gte-Qwen2-1.5B-instruct', trust_remote_code=True)    """def __init__(
            self,
            heads,  # 自注意力的头数
            head_dim,  # 单头embed_size
            embed_dim,  # hidden_size
            ffn_expand=4,  # FFN中上采样比例
            ffn_dropout=0.0,
            attn_dropout=0.0,
            decoder_layers=4,
            max_seq_len=512,
            vocab_size=-1  # 词库规模
    ):
        super(ChatLLM, self).__init__()
        self.heads = heads
        self.head_dim = head_dim
        self.embed_dim = embed_dim
        self.ffn_expand = ffn_expand
        self.ffn_dropout = ffn_dropout
        self.attn_dropout = attn_dropout
        self.decoder_layers = decoder_layers
        self.max_seq_len = max_seq_len
        self.vocab_size = vocab_size
        self.token_embedding = torch.nn.Embedding(vocab_size, embedding_dim=embed_dim)
        self.position_embedding = torch.nn.Embedding(max_seq_len, embedding_dim=embed_dim)
        self.decoders = nn.Sequential(
            *[DecoderLayer(heads, head_dim, embed_dim, ffn_expand, ffn_dropout, attn_dropout, max_seq_len) for _ in range(decoder_layers)]
            )
        # 自回归生成head
        self.lm_head = nn.Linear(embed_dim, vocab_size)
        self.temp = nn.Parameter(torch.Tensor([0.1]))
        self.apply(init_weights)
        pass

    def from_pretrain(self, pretrain):
        state_dict = torch.load(pretrain, map_location='cpu')
        self.load_state_dict(state_dict, strict=True)
        self.to(self.device)
        return self

    @property
    def device(self):
        if torch.cuda.is_available():
            return torch.device('cuda')
        return torch.device('cpu')

    def forward(self, input_ids, attention_mask, labels=None):
""":param input_ids: (N, seqLen):param attention_mask: (N, seqLen):param labels: 未进行了label_shift:return:        """# (N, seqLen, embed_dim)
        # print(f"======**** vocab_size={self.vocab_size}, input_ids.device={input_ids.shape}, device={input_ids.shape}||token_embedding.device={self.token_embedding.weight.shape}||position_embedding.device={self.position_embedding.weight.shape}")
        # raise NotImplementedError
        token_embedding = self.token_embedding(input_ids)
        position_idx = torch.arange(0, input_ids.size(1), dtype=torch.long, device=self.device).unsqueeze(0)
        position_embedding = self.position_embedding(position_idx)
        x = token_embedding + position_embedding
        for decoder in self.decoders:
            x = decoder(x, attention_mask)
        logits = self.lm_head(x) / self.temp  # (N, seqLen, vocab_size)
        loss = None
        if labels is not None:
            assert labels.size(-1) == input_ids.size(-1)
            labels = labels.to(logits.device)
            shifted_logits = logits[:, :-1, :].contiguous()  # [0, 1, 2, ...,n-1]
            shifted_labels = labels[:, 1:].contiguous()  # [1, 2, 3, ...,n]
            shifted_logits = shifted_logits.view(-1, self.vocab_size)
            shifted_labels = shifted_labels.view(-1)
            loss = torch.nn.functional.cross_entropy(shifted_logits, shifted_labels)
        return logits, loss

```

### generate

生成过程就是nextTokenPrediction的过程，通过不断迭代生成，直至达到最大生成数量或者遇到[EOS]为止。选词过程：

- 选取概率最高的词；
- 通过温度系数，改变分布的平滑性，控制多样性和随机性；
- 通过TopK的多项式采样，增加生成内容的多样性。

```
@torch.inference_mode()
def generate(self, input_ids, attention_mask, max_generate_tokens=100, temperature=1.0, **kwargs):
"""    生成过程计聊天的回答过程, 将进行逐个token生成，直到出现EOS或者达到最大长度限制为止:param input_ids::param attention_mask::param max_generate_tokens::param temperature::return: (N, generated_token_nums)    """top_k = 8
    full_token_id = input_ids[..., :self.max_seq_len]
    attention_mask = attention_mask[..., :self.max_seq_len]
    for _ in range(max_generate_tokens):
        logits, _ = self.forward(full_token_id, attention_mask, None)
        next_token_logits = logits[:, -1, :] / temperature  # (N, vocab_size)
        tops, _ = torch.topk(next_token_logits, 500, dim=-1)
        next_token_logits = next_token_logits.masked_fill(next_token_logits < tops[:, [-1]], -1e9)
        # (N, vocab_size)
        next_token_logits = torch.nn.functional.softmax(next_token_logits, dim=-1)
        # 获取topK的nex_token作为候选池, 并进行采样
        tops, _ = torch.topk(next_token_logits, top_k, dim=-1)
        next_token_logits = next_token_logits.masked_fill(next_token_logits < tops[:, [-1]], 0)
        # (N, 1)
        next_token_id = torch.multinomial(next_token_logits, 1)
        full_token_id = torch.concat([full_token_id, next_token_id], dim=-1)
    return full_token_id

```

测试未训练状态下的生成，在没有训练的情况下，生成结果完全混乱：

```
import os
import torch
import argparse
from omegaconf import OmegaConf
from trainer import MyTrainer
from models.chat import ChatLLM
from models.tokenizer import tokenizer
print(torch.cuda.is_available())

cfg_path = 'models/config.yaml'
conf = OmegaConf.load(cfg_path)
model_config = OmegaConf.to_container(conf.trainer_config.model_config)
model = ChatLLM(**model_config).to('cuda')
model.eval()
input_text = '诗词题目：独坐窗台'
full_tokens = model.chat(input_text, tokenizer, max_generate_tokens=50, temperature=0.8)
print(full_tokens)

[Output1]:
诗词题目：独坐窗台得好石头石头 WAY石头石头石头_extractor샐 féregions石头石头 WAY石头石头regions col石头黑洞石头黑洞石头regions_extractorregions_extractor得好黑洞샐regions col_extractor石头石头石头石头 col石头黑洞_extractor_extractor샐黑洞샐石头샐石头石头regions

[Output2]:
诗词题目：独坐窗台)'),
)'),
)'),
(',');
 najwięks(',');
 bs blatantly(',');
.Fire.Fire bs)'),
.Fire(',');
.Fire Northern(',');
 najwięks)'),
 blatantly design najwięks bs)'),
 Northern.Fire blatantly design blatantly Northern Northern.Fire.Fire design design najwięks(',');
 design blatantly Northern(',');
 bs Northern design blatantly design najwięks Northern najwięks

```

# 训练（10min）

## 数据

选择中文古诗词作为预料，数据内容照抄如下:

```
{"content": "诗词题目名：赠司空张公安道挽词三首\n孤高出世学，豪迈谪仙人。\n早岁犹和俗，中年自识真。\n定余时发照，尘尽四无邻。\n闻道骑箕尾，还应事玉宸。"}
{"content": "诗词题目名：芙蓉六言四首\n月地不离人世，花城岂必仙家。\n且容康节向月，不羡曼卿主花。"}
{"content": "诗词题目：泛镜湖南溪\n乘兴入幽栖，舟行日向低。岩花候冬发，谷鸟作春啼。\n沓嶂开天小，丛篁夹路迷。犹闻可怜处，更在若邪溪。"}
{"content": "诗词标题： 奉陪舅丈祠书彦平主簿兄游慧聚终日小饮上方\n凭高情不浅，尽日倚胡床。\n云散他山雨，风来古殿香。\n一川横晚照，重阁枕秋光。\n欲写无穷意，诗凡不擅场。"}

```

查看数据

```
from models.tokenizer import tokenizer
from models.dataset import PoemDataset, DATA_FILE, DataLoader
ds = PoemDataset(tokenizer, 512, DATA_FILE)
dataloader = DataLoader(
    ds,
    batch_size=4,
    collate_fn=ds.collate_func
)
for i, batch in enumerate(dataloader):
    print(f"-- batch{i}: input_ids.shape= {batch['input_ids'].shape}, attention_mask.shape= {batch['attention_mask'].shape}")
    if i >= 10:
        break

-- batch0: input_ids.shape= torch.Size([4, 56]), attention_mask.shape= torch.Size([4, 56])
-- batch1: input_ids.shape= torch.Size([4, 130]), attention_mask.shape= torch.Size([4, 130])
-- batch2: input_ids.shape= torch.Size([4, 78]), attention_mask.shape= torch.Size([4, 78])
-- batch3: input_ids.shape= torch.Size([4, 167]), attention_mask.shape= torch.Size([4, 167])
-- batch4: input_ids.shape= torch.Size([4, 132]), attention_mask.shape= torch.Size([4, 132])
-- batch5: input_ids.shape= torch.Size([4, 73]), attention_mask.shape= torch.Size([4, 73])
-- batch6: input_ids.shape= torch.Size([4, 75]), attention_mask.shape= torch.Size([4, 75])
-- batch7: input_ids.shape= torch.Size([4, 244]), attention_mask.shape= torch.Size([4, 244])
-- batch8: input_ids.shape= torch.Size([4, 78]), attention_mask.shape= torch.Size([4, 78])
-- batch9: input_ids.shape= torch.Size([4, 64]), attention_mask.shape= torch.Size([4, 64])
-- batch10: input_ids.shape= torch.Size([4, 280]), attention_mask.shape= torch.Size([4, 280])

```

Note: TokenLength是很昂贵的，因此Batch内采用最大长度的样本作为Batch的长度，而不是使用统一的max_seq_len。在LLM批量推理中，通常会将样本按照token长度从短到长排序后再切块，目的是让Batch内样本TokenLength尽可能接近。

## 训练Loss

[https://bytedance.larkoffice.com/space/api/box/stream/download/asynccode/?code=OTQ5NjNmNTc2MTkxYTVjODQ0N2VmYzY2ODEzM2IxMDRfdVBrOTZjVE82THZzOGhiSXFaUjFna2xad3RZSVpmdjJfVG9rZW46QnJHcmJxZnJUbzdqMWJ4T3NaSGNFSjdFbnllXzE3MjY5MTgzNDc6MTcyNjkyMTk0N19WNA](https://bytedance.larkoffice.com/space/api/box/stream/download/asynccode/?code=OTQ5NjNmNTc2MTkxYTVjODQ0N2VmYzY2ODEzM2IxMDRfdVBrOTZjVE82THZzOGhiSXFaUjFna2xad3RZSVpmdjJfVG9rZW46QnJHcmJxZnJUbzdqMWJ4T3NaSGNFSjdFbnllXzE3MjY5MTgzNDc6MTcyNjkyMTk0N19WNA)

## 生成测试

```
import os
import torch
import argparse
from omegaconf import OmegaConf
from trainer import MyTrainer
print(torch.cuda.is_available())

cfg_path = 'models/config.yaml'
conf = OmegaConf.load(cfg_path)
trainer_config = OmegaConf.to_container(conf.trainer_config)
trainer = MyTrainer(**trainer_config)
model = trainer.model
ckpt_path = '/mnt/bn/account-mllm-lf1/BLM_TRAIN/tutorial/ckpt/ckpt_ep1_gs12500.pt'
model = model.from_pretrain(ckpt_path)
tokenizer = trainer.tokenizer
model = model.eval()

input_text = '诗词题目：独坐窗台'
full_tokens = model.chat(input_text, tokenizer, max_generate_tokens=50, temperature=0.8)
print(full_tokens)

```

[https://bytedance.larkoffice.com/space/api/box/stream/download/asynccode/?code=ZGY4MjY5Y2E3YzZjODMwYTI5OWI2ZjE5OWI1Mzc4MTJfVVhReE8wMzRRMnVyTXlVa2IxQXRJcVlTeFR0YVEyazJfVG9rZW46QmgwN2JsOFd1b050OUR4Y1JUYWNxdjNqbldkXzE3MjY5MTgzNDc6MTcyNjkyMTk0N19WNA](https://bytedance.larkoffice.com/space/api/box/stream/download/asynccode/?code=ZGY4MjY5Y2E3YzZjODMwYTI5OWI2ZjE5OWI1Mzc4MTJfVVhReE8wMzRRMnVyTXlVa2IxQXRJcVlTeFR0YVEyazJfVG9rZW46QmgwN2JsOFd1b050OUR4Y1JUYWNxdjNqbldkXzE3MjY5MTgzNDc6MTcyNjkyMTk0N19WNA)

[https://bytedance.larkoffice.com/space/api/box/stream/download/asynccode/?code=MTNmNDIyZmQ2OTViYTM3ODE5ZjQyZDQ5NzE5ZjhhZTBfczdQdUwyU3NCRENYb3FYMmFmVXJMbXk3cmg4SUE0RXJfVG9rZW46WEV3eWJ1UUV2b0V6dTZ4WVNWbWNYMlVabllCXzE3MjY5MTgzNDc6MTcyNjkyMTk0N19WNA](https://bytedance.larkoffice.com/space/api/box/stream/download/asynccode/?code=MTNmNDIyZmQ2OTViYTM3ODE5ZjQyZDQ5NzE5ZjhhZTBfczdQdUwyU3NCRENYb3FYMmFmVXJMbXk3cmg4SUE0RXJfVG9rZW46WEV3eWJ1UUV2b0V6dTZ4WVNWbWNYMlVabllCXzE3MjY5MTgzNDc6MTcyNjkyMTk0N19WNA)

[https://bytedance.larkoffice.com/space/api/box/stream/download/asynccode/?code=NWE5MzI0Zjk5MjZkMzAzMDAzMGVkMWU0NGNhNTJhNzFfdTlhRkVHU2dibDd5Z3RmN2tHOFpxcWpQVUl2RzVCNW5fVG9rZW46TFVSQmJKWDlPb0c1ZTJ4dHNMdGNJVWdDbkpoXzE3MjY5MTgzNDc6MTcyNjkyMTk0N19WNA](https://bytedance.larkoffice.com/space/api/box/stream/download/asynccode/?code=NWE5MzI0Zjk5MjZkMzAzMDAzMGVkMWU0NGNhNTJhNzFfdTlhRkVHU2dibDd5Z3RmN2tHOFpxcWpQVUl2RzVCNW5fVG9rZW46TFVSQmJKWDlPb0c1ZTJ4dHNMdGNJVWdDbkpoXzE3MjY5MTgzNDc6MTcyNjkyMTk0N19WNA)

# 问题思考（2min）

其实LLM和早期的BertModel并没有本质上的区别，其最大的不同在于更大的预训练数据量、更多的资源、更长的训练时间、更丰富的数据（In-Context Learning、RLHF），从GPT3.5开始效果上引起了较大的轰动，主要在于LLM能够提供很好的One-shot Learning（世界知识学习、语法学习、人类偏好对齐）和强大的Few-shot Learning。当前大家追求的无非是更大的参数、更多的语料、更长的Context支持能力，由此带来很多功能上的创新研究。

Scaling law: 实验经验揭示模型大小、数据规模、收敛效果直接的关系。

[https://bytedance.larkoffice.com/space/api/box/stream/download/asynccode/?code=ZjEzNGVjZDYyNWNiMWM3NmNhYTg2MmY2NTdkMDliN2JfbXpTNjBVMlJLN25ySHdyWEZtYnhIVUUwbFdrU3lPbGtfVG9rZW46VzQ4cGJPOExGb3V0Z1d4Z0psdmNQaG9Qbm5oXzE3MjY5MTgzNDc6MTcyNjkyMTk0N19WNA](https://bytedance.larkoffice.com/space/api/box/stream/download/asynccode/?code=ZjEzNGVjZDYyNWNiMWM3NmNhYTg2MmY2NTdkMDliN2JfbXpTNjBVMlJLN25ySHdyWEZtYnhIVUUwbFdrU3lPbGtfVG9rZW46VzQ4cGJPOExGb3V0Z1d4Z0psdmNQaG9Qbm5oXzE3MjY5MTgzNDc6MTcyNjkyMTk0N19WNA)

LLM的特殊点：

| **问题** | **详情** |
| --- | --- |
| 位置编码长度固定问题 | 使用可变位置编码，解决不定长文本长度问题，且可扩展，如：RoPE |
| 长序列问题 | 长序列会带来计算量和内存暴增的问题，提出SparseAttention、SlideWindowAttention、MultiQueryAttention、GroupQueryAttention、KVCache |
| 训练资源不够 | 模型并行、数据并行、张量并行、ZEROs、GradientCheckpoint |
| 训练推理加速 | FlashAttention、PageAttention、GPTQ、 |
| 长文本遗忘，世界知识更新 | RAG、SparseAttention |

代码地址：https://code.byted.org/risk_control/llm_tutorial_primary