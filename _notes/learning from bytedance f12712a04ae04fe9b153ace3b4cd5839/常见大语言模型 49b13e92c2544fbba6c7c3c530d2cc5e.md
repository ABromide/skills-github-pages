# 常见大语言模型

# 模型训练Overview

1. Pre-training
2. Supervised Finetuning
    1. Unsloth、FlashAttn加速推理
3. Reinforcement learning from human Feedback
    1. Reward Modeling
    2. Reinforcement Learning（PPO与DPO）
4. Model Quantization（AWQ、GPTQ、 BNB）

VIT：

[【深度学习】详解 Vision Transformer (ViT)-CSDN博客](https://blog.csdn.net/qq_39478403/article/details/118704747?spm=1001.2014.3001.5506)

Clip：

[CLIP改进工作串讲（上）-CSDN博客](https://blog.csdn.net/weixin_44966641/article/details/127340680?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522172275845616800227415261%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&request_id=172275845616800227415261&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-1-127340680-null-null.142^v100^pc_search_result_base9&utm_term=clip%E6%A8%A1%E5%9E%8B%E4%B8%B2%E8%AE%B2&spm=1018.2226.3001.4187)

## llama2:

![image.png](%E5%B8%B8%E8%A7%81%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%2049b13e92c2544fbba6c7c3c530d2cc5e/image.png)

1. RoPE:[transformer位置编码-RoPE](https://bytedance.larkoffice.com/wiki/YvppwCqy4imVcxkTEAkcAzZZn3z)
2. RMSNorm:[【NLP】理解 Llama2:KV 缓存、分组查询注意力、旋转嵌入等-CSDN博客](https://blog.csdn.net/sikh_0529/article/details/134375318)
3. 键值KV缓存:因为逐个输出token的时候，有重复计算的要求，可以提前放在cache中
4. 分组查询注意力：TODO

## T5:encoder-decoder-based model

1. PE：相对位置编码https://juejin.cn/post/7107516511824117790
    1. Transformer 使用正余弦函数的位置编码，BERT 使用的是学习到的位置嵌入，而本文使用的是相对位置嵌入。使用了一种类似于量化的方法进行处理外推的问题，将一定范围的相对距离的特征映射到同一个位置
    
    T5并没有在输入的input embedding之后加position embedding，而是在Encoder的第一层的Self-attention计算Q和K乘积之后加入了一个relative position embbedding，也就是在计算softmax之前。
    

## DeepSeek-V2（混合专家 (MoE) 语言模型）→ 没看懂ing

https://bytetech.info/articles/7370923793537040447?from=message_bot&sender_type=101&message_id=7392049590398599177&column=like#QEnPdxKbNo6RyOxov42cCUZinJfhttps://blog.csdn.net/qq_28385535/article/details/139401076

![Untitled](%E5%B8%B8%E8%A7%81%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%2049b13e92c2544fbba6c7c3c530d2cc5e/Untitled.png)

### 2.1 Multi-Head Latent Attention: Boosting Inference Efficiency

- 2.1.1 Preliminaries: Standard Multi-Head Attention
    
    LLM模型多采用多头注意力机制，传统推理时，每次都需要计算t次q、k、v（t表示第t个token），但不难发现，q只需计算当前token即可，前t-1个token的k和v都是重复计算，所以我们可以将k和v缓存起来，大大减少计算量。
    
    [https://bytedance.larkoffice.com/space/api/box/stream/download/asynccode/?code=ZjYzMGFkNDk1NTY1NGU1YTQwNWUzMTM2ODQ1OTQ4YmVfN1Vva1NlbXhRZUhFNWJta1Bzd0VoWFIwWTU5eHVlcExfVG9rZW46SkhWMmJ2N0pNb2wyaHp4eGFVOGNaMjlWbmlnXzE3MjE5NjA0NDI6MTcyMTk2NDA0Ml9WNA](https://bytedance.larkoffice.com/space/api/box/stream/download/asynccode/?code=ZjYzMGFkNDk1NTY1NGU1YTQwNWUzMTM2ODQ1OTQ4YmVfN1Vva1NlbXhRZUhFNWJta1Bzd0VoWFIwWTU5eHVlcExfVG9rZW46SkhWMmJ2N0pNb2wyaHp4eGFVOGNaMjlWbmlnXzE3MjE5NjA0NDI6MTcyMTk2NDA0Ml9WNA)
    
    传统的KV cache通过缓存key、value，从而不必每次重新计算，提高推理速度。但是在推理过程中，随着序列长度的不断增加，KV cache会越来越大，访问的开销也越来越大，已然成为了目前限制推理速度的瓶颈。
    
    目前解决方法：MQA、GQA
    
    [https://bytedance.larkoffice.com/space/api/box/stream/download/asynccode/?code=MTdjNTg2ZTZkMmU1NWRiYTJjNTYwYjAxZjQyNzNjMjFfZWYwV0JkRGZaRWwweTV4c3RtcjlIYXp0M0lmczA0U2xfVG9rZW46VU03emJXS0xvb3RGUUl4bTA1QmNFYko0blAyXzE3MjE5NjA0NDI6MTcyMTk2NDA0Ml9WNA](https://bytedance.larkoffice.com/space/api/box/stream/download/asynccode/?code=MTdjNTg2ZTZkMmU1NWRiYTJjNTYwYjAxZjQyNzNjMjFfZWYwV0JkRGZaRWwweTV4c3RtcjlIYXp0M0lmczA0U2xfVG9rZW46VU03emJXS0xvb3RGUUl4bTA1QmNFYko0blAyXzE3MjE5NjA0NDI6MTcyMTk2NDA0Ml9WNA)
    
    - MHA：为每个注意力头都存一组k、v，内存开销：$$2\times n_{d}\times n_{token}\times d_{h}\times l$$，不存在精度损失
    - MQA：所有注意力头共享一组k、v，内存开销：$$2\times n_{token}\times d_{h}\times l$$，精度损失较大
    - GQA：对注意力头进行分组，每组共享一组k、v，内存开销：$$2\times n_{g}\times n_{token}\times d_{h}\times l$$，这是MHA和MQA的折中方案，精度损失也介于MHA和GQA之间
    
- Low-Rank Key-Value Joint Compression
    
    

## Chatglm架构

https://bytetech.info/articles/7299674106918600738?from=lark_all_search#doxcnnEwubcERGujchgFPKnrtxb

| 模型 | 模型结构 | 位置编码 | 激活函数 | Layer norm |
| --- | --- | --- | --- | --- |
| LLaMA | Casual decoder | RoPE | SwiGLU | Pre RMS Norm |
| ChatGLM-6B | Prefix decoder | RoPE | GeGLU | Post Deep Norm |
| ChatGLM2-6B | Casual decoder | RoPE | SwiGLU | Post Deep Norm |
| Bloom | Casual decoder | ALiBi | GeLU | Pre Layer Norm |

**tokenizer**

| **模型** | **词表大小** | **中文平均token数** | **英文平均token数** | **中文处理时间(s)** | **英文处理时间(s)** |
| --- | --- | --- | --- | --- | --- |
| LLaMA | 32000 | 1.45 | 0.25 | 12.6 | 19.4 |
| ChatGLM-6B | 130528 | 0.55 | 0.19 | 15.91 | 20.84 |
| Bloom | 250880 | 0.53 | 0.22 | 9.87 | 15.6 |
1. Causal Decoder

Causal Decoder架构的典型代表就是GPT系列模型，使用的是单向注意力掩码，以确保每个输入token只能注意到过去的token和它本身，输入和输出的token通过Decoder以相同的方式进行处理。在下图中，灰色代表对应的两个token互相之间看不到，否则就代表可以看到。例如，”Survery”可以看到前面的“A”，但是看不到后面的“of”。Causal Decoder的sequence mask矩阵是一种典型的下三角矩阵。

[https://bytedance.larkoffice.com/space/api/box/stream/download/asynccode/?code=NzYyYWYxNTM2MWFhZjI4MDFiYzk4NjU2ODQ4ZTFjMGNfa2lXUUZUTGF0QXdwNW5OYWRDdlVucDJKemh6WnBBMVJfVG9rZW46VGFtRGJieElobzVVQjJ4ZkVPWmN0bzhPbmRiXzE3MjE5ODczMzk6MTcyMTk5MDkzOV9WNA](https://bytedance.larkoffice.com/space/api/box/stream/download/asynccode/?code=NzYyYWYxNTM2MWFhZjI4MDFiYzk4NjU2ODQ4ZTFjMGNfa2lXUUZUTGF0QXdwNW5OYWRDdlVucDJKemh6WnBBMVJfVG9rZW46VGFtRGJieElobzVVQjJ4ZkVPWmN0bzhPbmRiXzE3MjE5ODczMzk6MTcyMTk5MDkzOV9WNA)

1. Encoder-Decoder

Transformer最初被提出来的时候采用的就是Encoder-Decoder架构，模型包含两部分Encoder和Decoder，两部分参数独立。其中Encoder将输入序列处理为一种中间表示，而Decoder则基于中间表示自回归地生成目标序列。

[https://bytedance.larkoffice.com/space/api/box/stream/download/asynccode/?code=ZWVlMGMwNzIyZTYyMmJlZDk2YWNmZmRiZDYxMGFiZmZfSFRpMzk0UmNIQUlxWG1Cd3dEY21zcXZYZVo4R3JUYWZfVG9rZW46T1J4Y2I0Rkpyb1FoMHR4ZnQwTGM3ejBXbmhoXzE3MjE5ODczMzk6MTcyMTk5MDkzOV9WNA](https://bytedance.larkoffice.com/space/api/box/stream/download/asynccode/?code=ZWVlMGMwNzIyZTYyMmJlZDk2YWNmZmRiZDYxMGFiZmZfSFRpMzk0UmNIQUlxWG1Cd3dEY21zcXZYZVo4R3JUYWZfVG9rZW46T1J4Y2I0Rkpyb1FoMHR4ZnQwTGM3ejBXbmhoXzE3MjE5ODczMzk6MTcyMTk5MDkzOV9WNA)

1. Prefix Decoder
    
    Prefix Decoder架构也被称为non-causal Decoder架构，它的注意力机制和Encoder-Decoder很接近，也是输入部分采用双向注意力，而输出部分采用单向注意力。
    
    [https://bytedance.larkoffice.com/space/api/box/stream/download/asynccode/?code=NTg1NDdmMTAxMGE5ZjdkYmYxZmFmMGM1OTEwNTRhOGRfU0x1a1pDRG9SbTd0VFdLbjFnSUxCc2FCakFLYjNtMmRfVG9rZW46VDVhUmJoZ0xKbzdHRDF4ZzRIZ2NxYmJLbjJnXzE3MjE5ODczMzk6MTcyMTk5MDkzOV9WNA](https://bytedance.larkoffice.com/space/api/box/stream/download/asynccode/?code=NTg1NDdmMTAxMGE5ZjdkYmYxZmFmMGM1OTEwNTRhOGRfU0x1a1pDRG9SbTd0VFdLbjFnSUxCc2FCakFLYjNtMmRfVG9rZW46VDVhUmJoZ0xKbzdHRDF4ZzRIZ2NxYmJLbjJnXzE3MjE5ODczMzk6MTcyMTk5MDkzOV9WNA)
    

| **模型架构** | **代表LLM** | **注意力机制** | **是否属于Decoder-Only** |
| --- | --- | --- | --- |
| Causal Decoder | GPT3/ChatGPT/ChatGLM2-6B | 纯单向 | YES |
| Encoder-Decoder | Flan-T5 | 输入双向，输出单向 | NO |
| Prefix Decoder | GLM-130B/ChatGLM-6B | 输入双向，输出单向 | YES |

在训练目标上，ChatGLM-6B的训练任务是自回归文本填空。相比于采用causal decoder-only结构的大语言模型，采用prefix decoder-only结构的ChatGLM-6B存在一个劣势：训练效率低。causal decoder结构会在所有的token上计算损失，而prefix decoder只会在输出上计算损失，而不计算输入上的损失。在有相同数量的训练tokens的情况下，prefix decoder要比causal decoder的效果差，因为训练过程中实际用到的tokens数量要更少。

下面对 T5、BERT 和 GPT 模型在各个维度进行更深入的比较：

### **1. 标记化和词汇**

- **BERT**：使用 WordPiece 标记化，词汇量约为 30,000 个标记。
- **GPT**：采用具有大词汇量的字节对编码（BPE）（例如，GPT-3 的词汇量为175,000）。
- **T5**：利用 SentencePiece 标记化，将文本视为原始文本，不需要预先分段的单词。

### **2. 预训练目标**

- **BERT**：掩码语言模型（MLM）和下一句预测（NSP）。
- **GPT**：因果语言模型（CLM），其中每个标记预测序列中的下一个标记。
- **T5**：使用去噪目标，其中随机的文本范围被哨兵标记替换，并且模型学习重建原始文本。

### **3. 输入表示**

- **BERT**：将 Token、Segment 和 Positional Embedding 组合起来表示输入。
- **GPT**：令牌和位置嵌入相结合（没有段嵌入，因为它不是为句子对任务设计的）。
- **T5**：仅在注意操作期间添加相对位置编码的令牌嵌入。

### **4.注意力机制**

- **BERT**：使用绝对位置编码并允许每个标记关注左侧和右侧的所有标记（双向关注）。
- **GPT**：也使用绝对位置编码，但仅限制对先前标记的注意力（单向注意力）。
- **T5**：实现变压器的变体，它使用相对位置偏差而不是位置嵌入。

### **5. 模型架构**

- **BERT**：具有多层变压器块的纯编码器架构。
- **GPT**：仅解码器架构，也具有多层，但专为生成任务而设计。
- **T5**：编码器-解码器架构，其中编码器和解码器均由变压器层组成。

### **6. 微调方法**

- **BERT**：根据需要使用附加输出层调整下游任务的预训练模型的最终隐藏状态。
- **GPT**：在变压器顶部添加一个线性层，并使用相同的因果语言建模目标对下游任务进行微调。
- **T5**：将所有任务转换为文本到文本格式，其中模型经过微调以从输入序列生成目标序列。

### **7. 训练数据和规模**

- **BERT**：接受过 BooksCorpus 和英语维基百科的培训。
- **GPT**：GPT-2 和 GPT-3 已经在从互联网上提取的不同数据集上进行了训练，GPT-3 则在一个更大的名为 Common Crawl 的语料库上进行了训练。
- **T5**：在“Colossal Clean Crawled Corpus”上进行训练，这是 Common Crawl 的大型且干净的版本。

### **8. 上下文和双向性的处理**

- **BERT**：旨在同时理解两个方向的上下文。
- **GPT**：经过训练可以向前（从左到右）理解上下文。
- **T5**：可以在编码器中建模双向上下文，在解码器中建模单向上下文，适用于序列到序列任务。

### **9. 下游任务的适应性**

- **BERT**：需要特定于任务的头层并针对每个下游任务进行微调。
- **GPT**：本质上是生成性的，可以在对其结构进行最小改变的情况下提示执行任务。
- **T5**：将每个任务视为“文本到文本”问题，使其具有固有的灵活性并适应新任务。

### **10. 可解释性和可解释性**

- **BERT**：双向性质提供了丰富的上下文嵌入，但可能更难以解释。
- **GPT**：单向上下文可能更容易理解，但缺乏双向上下文的深度。
- **T5**：编码器-解码器框架提供了清晰的处理步骤分离，但由于其生成性质，分析起来可能很复杂。