# nlpç›¸å…³ä»“åº“

[GitHub - Hannibal046/Awesome-LLM: Awesome-LLM: a curated list of Large Language Model](https://github.com/Hannibal046/Awesome-LLM)

[GitHub - huybery/Awesome-Code-LLM: ğŸ‘¨ğŸ’» An awesome and curated list of best code-LLM for research.](https://github.com/huybery/Awesome-Code-LLM)

https://github.com/liaokongVFX/LangChain-Chinese-Getting-Started-Guide/tree/main

langchainï¼šå¯ä»¥å°†è¾“å‡ºçš„é“¾è·¯å˜æˆä¸€ä¸ªchainè¿›è¡Œè¾“å‡ºï¼Œå°†æ€è€ƒè¿‡ç¨‹ä¹Ÿè¾“å‡ºå‡ºæ¥ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œï¼ˆzapierï¼‰å¯ä»¥è½½å…¥æ›´å¤šçš„apiï¼Œå¦‚ducumentã€youtubeç­‰ï¼Œï¼ˆChroma**/**pineconeï¼‰æ„å»ºæœ¬åœ°çŸ¥è¯†åº“ã€‚æ¯”å¦‚ä¹‹å‰ç”¨æ¥æ€»ç»“è§†é¢‘å†…å®¹çš„æœºå™¨äººå°±æ˜¯æ ¹æ®è¿™ä¸ªå·¥ä½œæ„å»ºçš„ã€‚ï¼ˆSerpapiï¼‰ç”¨æ¥æ¥å…¥Googleçš„æœç´¢åŠŸèƒ½ã€‚

# Finetune Toolsï¼š

[GitHub - hiyouga/LLaMA-Factory: Unify Efficient Fine-Tuning of 100+ LLMs](https://github.com/hiyouga/LLaMA-Factory)

# SFT paperï¼š

ç°æœ‰çš„å¤§éƒ¨åˆ†è¿›è¡ŒSFTçš„å·¥ä½œï¼Œéƒ½æ˜¯é€šè¿‡æ„å»ºä¸€ä¸ªåˆé€‚çš„prompt/instructionå»æ„å»ºä¸€äº›ç‹¬æœ‰çš„æç¤ºæ–¹å‘/æˆ–è€…æ‰©å……æ•°æ®é‡ã€‚è¿™æ ·çš„æ–¹æ³•æ¥è¿›è¡Œç”¨å¤§æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›æ¥åœ¨æé«˜æŸä¸€ä¸ªç‰¹æ®Šä»»åŠ¡ä¸Šçš„å¯è¡Œæ€§ã€‚

[https://arxiv.org/abs/2212.10560](https://arxiv.org/abs/2212.10560)ã€self-instructã€‘https://zhuanlan.zhihu.com/p/614916562

é€šè¿‡ä¸€ä¸ªself- instructçš„æ–¹æ³•è¿›è¡ŒæŒ‡ä»¤çš„ç”Ÿæˆã€‚åº”è¯¥æ˜¯è‡ªå·±é€šè¿‡instructç”Ÿæˆä¸€å †æ–°çš„instructçš„inputå’Œoutputæ•°æ®æ¥ï¼Œå†æŠŠè¿™äº›æ•°æ®æ‹¿å›æ¥ç»™è‡ªå·±è¿›è¡Œè®­ç»ƒï¼Œç›¸å½“äºè‡ªå·±çš„æç¤ºå’Œæ•°æ®é›†éƒ½è‡ªå·±é€ ã€‚https://github.com/yizhongw/self-instruct/tree/main

```
# 1. Generate instructions from the seed tasks
./scripts/generate_instructions.sh

# 2. Identify whether the instruction represents a classification task or not
./scripts/is_clf_or_not.sh

# 3. Generate instances for each instruction
./scripts/generate_instances.sh

# 4. Filtering, processing, and reformatting
./scripts/prepare_for_finetuning.sh

```

[https://arxiv.org/abs/2304.12244](https://arxiv.org/abs/2304.12244)**WizardLM**

[https://arxiv.org/abs/2304.03277](https://arxiv.org/abs/2304.03277)[https://arxiv.org/abs/2305.11206](https://arxiv.org/abs/2305.11206)[https://arxiv.org/abs/2305.14233](https://arxiv.org/abs/2305.14233)[https://arxiv.org/pdf/2306.02707](https://arxiv.org/pdf/2306.02707)[https://arxiv.org/abs/2308.06259](https://arxiv.org/abs/2308.06259)[https://arxiv.org/abs/2308.10792](https://arxiv.org/abs/2308.10792)[https://arxiv.org/pdf/2401.01335](https://arxiv.org/pdf/2401.01335)

1. Instruction Tuning for Large Language Models: A Survey

è¿™ç§æ¨¡å¼ä¸‹çš„ç”¨æ¥finetuneçš„æ•°æ®ä¸€èˆ¬æ˜¯

- a natural language text sequence to specify the task
- an optional input which provides supplementary information for context;
- an anticipated output based on the instruction and the input.ã€

# MoCo

https://www.cvmart.net/community/detail/5179#circle=on