# 模型量化

[模型量化方法](https://www.zhihu.com/question/627484732/answer/3261671478)

没有量化的情况下，显存占用：https://bytetech.info/articles/7393981108456521740?from=lark_all_search#GYnUdYuQXo4hxaxadnelr1gwgcd

[LLM - 训练、推理显存分析](%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96%20d78da432d0c64ed991d5e2b4f500a710/LLM%20-%20%E8%AE%AD%E7%BB%83%E3%80%81%E6%8E%A8%E7%90%86%E6%98%BE%E5%AD%98%E5%88%86%E6%9E%90%206fc4a51f775241b3a0e1d88a1a539e7d.md)

https://mp.weixin.qq.com/s/UVXQdTt9Ala1erMGBwTHOQ

# 量化对象

![截屏2024-07-29 17.48.56.png](%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96%20d78da432d0c64ed991d5e2b4f500a710/%25E6%2588%25AA%25E5%25B1%258F2024-07-29_17.48.56.png)

在Transformer模型中，激活函数主要用于引入非线性，使得模型能够学习复杂的特征表示。Transformer模型中主要使用了以下几种激活函数：

1. **ReLU（Rectified Linear Unit）**：
    - 通常在前馈网络（Feed-Forward Neural Network, FFNN）中使用。ReLU是一种非常流行的激活函数，因为它计算简单且训练效率高。公式为：\[ f(x) = \max(0, x) \]
2. **GELU（Gaussian Error Linear Unit）**：
    - 在BERT及其衍生模型中，GELU被用作前馈网络的激活函数。GELU考虑了输入数据的分布特性，公式为：\[ f(x) = x \cdot \Phi(x) \]
    其中 \( \Phi \) 是标准正态分布的累积分布函数。
3. **Swish或Sigmoid线性单元（SiLU, also known as Swish）**：
    - 在一些变体中，如SiLU或Swish激活函数被用于替代ReLU或GELU。SiLU的公式为：\[ f(x) = x \cdot \sigma(x) \]
    其中 \( \sigma(x) \) 是Sigmoid函数。
4. **Mish**：
    - Mish是另一种较新的激活函数，它在某些Transformer变体中被使用。Mish的公式为：\[ f(x) = x \cdot \tanh(\text{softplus}(x)) \]
    其中 \( \text{softplus}(x) \) 是平滑的ReLU。

在Transformer的结构中，激活函数通常用在以下部分：

- **前馈网络（Feed-Forward Networks, FFNs）**：
    - Transformer中的每个编码器（Encoder）和解码器（Decoder）层都包含两个前馈网络，它们用于在自注意力机制之后进一步处理数据。这些前馈网络通常包含线性变换和非线性激活函数。
- **多头自注意力机制（Multi-Head Self-Attention）**：
    - 虽然自注意力机制本身不包含激活函数，但是自注意力的输出通常会通过一个线性层，这个线性层之后可能会跟一个激活函数，尽管在原始的Transformer模型中并没有这样做。
- **嵌入层（Embedding Layers）**：
    - 在某些情况下，词嵌入层或位置编码层之后可能会跟一个激活函数，但这在标准的Transformer模型中不常见。

值得注意的是，不同的Transformer模型变体可能会选择不同的激活函数，以适应特定的任务或提高模型性能。例如，BERT使用了GELU，而后来的一些模型可能会尝试使用SiLU或Mish等其他激活函数。

# AWQ

1. 先放大后量化

1. 关键参数保护→激活
    1. 在广泛的应用领域中，少数的关键因素往往决定了大部分的结果。这一原理在LLM中同样适用。大模型虽然存在几百上千亿的权重参数，但只有其中的少数权重贡献了绝大多数的模型效果。AWQ指出，权重并不是同等重要的，仅有1%的关键权重对模型效果的贡献占据了主导地位。保护这些关键权重，可以大大减少量化误差。
    2. **通过激活分布识别关键权重**
        1. 通过对transformer块的最终输出Out的构成成分分析，可以发现，被激活算子抑制位置的权重对输出Out值大小的影响几乎为零，对最终输出Out值贡献大的都是能被激活算子保留的位置的元素。
    

![截屏2024-08-14 10.38.14.png](%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96%20d78da432d0c64ed991d5e2b4f500a710/%25E6%2588%25AA%25E5%25B1%258F2024-08-14_10.38.14.png)

![image.png](%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96%20d78da432d0c64ed991d5e2b4f500a710/image.png)

1. 网格化搜索

https://zhuanlan.zhihu.com/p/685867596?utm_psn=1751997059807211520

# GPTQ

# BNB